{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPttXdjrXwNH",
        "outputId": "c2bca909-b5bd-4f64-ed42-89173d8843b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI is publicly accessible at: https://ec32-34-80-52-151.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "DARK_COLORS = [\"#1A0D26\", \"#351B4B\", \"#4F2871\", \"#693696\"]\n",
        "LIGHT_COLORS = [\"#9C69C9\", \"#B58ED7\", \"#CEB4E4\", \"#E6D9F2\"]\n",
        "\n",
        "# Apply nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "cloudinary.config(\n",
        "    cloud_name=\"dwd6kau8a\",\n",
        "    api_key=\"414118375842875\",\n",
        "    api_secret=\"99IAqTayxvBkd2aC5DVY1kj1jR0\"\n",
        ")\n",
        "\n",
        "# CORS middleware configuration\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "def plot_to_base64(fig):\n",
        "    \"\"\"Convert matplotlib plot to base64 string\"\"\"\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format='png', bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "    plt.close(fig)\n",
        "    return image_base64\n",
        "\n",
        "def is_likely_id_column(df, column):\n",
        "    \"\"\"Identifies if a column is likely an ID based on unique values ratio.\"\"\"\n",
        "    unique_ratio = df[column].nunique() / len(df)\n",
        "    return unique_ratio > 0.5  # ID-like columns have high uniqueness\n",
        "\n",
        "def generate_dynamic_recommendations(df, column):\n",
        "    \"\"\"Generates data-driven recommendations for a categorical column.\"\"\"\n",
        "    top_values = df[column].value_counts().head(3)\n",
        "    total_count = len(df)\n",
        "\n",
        "    insights = []\n",
        "    for i, (value, count) in enumerate(top_values.items(), 1):\n",
        "        percentage = count / total_count * 100\n",
        "        insights.append(f\"#{i}: '{value}' ({percentage:.1f}%)\")\n",
        "\n",
        "    # Dynamic recommendations based on patterns\n",
        "    if top_values.iloc[0] / total_count > 0.5:\n",
        "        recommendations = \"One value dominates—consider diversifying or investigating bias.\"\n",
        "    elif len(df[column].unique()) > 20:\n",
        "        recommendations = \"Many unique values—group similar categories for better insights.\"\n",
        "    else:\n",
        "        recommendations = \"Balanced distribution—use for segmentation & targeting.\"\n",
        "\n",
        "    return \"\\n\".join(insights) + \"\\n\\n\" + recommendations\n",
        "\n",
        "def generate_dynamic_categorical_insights(df, column):\n",
        "    \"\"\"Generates dynamic insights based on categorical distribution, including top 3 values.\"\"\"\n",
        "    category_counts = df[column].value_counts(normalize=True)\n",
        "    total_values = len(df)\n",
        "    unique_values = df[column].nunique()\n",
        "\n",
        "    top_category = category_counts.idxmax()\n",
        "    top_category_percentage = category_counts.max()\n",
        "\n",
        "    insights = []\n",
        "\n",
        "    if top_category_percentage > 0.5:\n",
        "        insights.append(f\"{column} is dominated by '{top_category}' ({top_category_percentage:.1%}).\\n\"\n",
        "                        f\"Consider diversifying strategies to balance market share.\")\n",
        "\n",
        "    if unique_values > 50 and top_category_percentage < 0.05:\n",
        "        insights.append(f\"{column} has {unique_values} unique categories, none dominant.\\n\"\n",
        "                        f\"Clustering (e.g., K-Means) may reveal hidden patterns.\")\n",
        "\n",
        "    if unique_values > 5 and top_category_percentage < 0.3:\n",
        "        insights.append(f\"{column} is well-distributed with {unique_values} unique values.\\n\"\n",
        "                        f\"Explore correlations with key business metrics like revenue.\")\n",
        "\n",
        "    if (category_counts < 0.01).sum() > unique_values * 0.5:\n",
        "        insights.append(f\"{column} has many low-frequency categories.\\n\"\n",
        "                        f\"Identify if these are niche products, seasonal trends, or data errors.\")\n",
        "\n",
        "    if unique_values < total_values * 0.05:\n",
        "        insights.append(f\"{column} likely represents key attributes like product types.\\n\"\n",
        "                        f\"Use this for targeted marketing and inventory optimization.\")\n",
        "\n",
        "    # Add the top 3 values and their percentages\n",
        "    top_3_values = category_counts.head(3)\n",
        "    top_3_text = \"\\n\".join([f\"{i+1}. '{val}' - {perc:.1%}\" for i, (val, perc) in enumerate(top_3_values.items())])\n",
        "\n",
        "    return \"\\n\".join(insights) + \"\\n\\nTop 3 Values:\\n\" + top_3_text if insights else f\"{column} contains meaningful business insights.\\n\\nTop 3 Values:\\n{top_3_text}\"\n",
        "\n",
        "def generate_numerical_insights(df, column):\n",
        "    \"\"\"Generates insights based on numerical distributions, skewness, and outliers.\"\"\"\n",
        "    insights = []\n",
        "    mean_val = df[column].mean()\n",
        "    median_val = df[column].median()\n",
        "    std_dev = df[column].std()\n",
        "    skewness = df[column].skew()\n",
        "    kurtosis = df[column].kurtosis()\n",
        "\n",
        "    insights.append(f\"Mean: {mean_val:.2f}, Median: {median_val:.2f}, Std Dev: {std_dev:.2f}\")\n",
        "\n",
        "    if abs(skewness) > 1:\n",
        "        insights.append(f\"Highly skewed (Skewness: {skewness:.2f})\")\n",
        "    elif abs(skewness) > 0.5:\n",
        "        insights.append(f\"Moderately skewed (Skewness: {skewness:.2f})\")\n",
        "    else:\n",
        "        insights.append(f\"Approximately Normal distribution (Skewness: {skewness:.2f})\")\n",
        "\n",
        "    if kurtosis > 3:\n",
        "        insights.append(f\"High peak and heavy tails (Kurtosis: {kurtosis:.2f})\")\n",
        "    elif kurtosis < -1:\n",
        "        insights.append(f\"Flatter distribution (Kurtosis: {kurtosis:.2f})\")\n",
        "\n",
        "    q1, q3 = df[column].quantile([0.25, 0.75])\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column]\n",
        "\n",
        "    if not outliers.empty:\n",
        "        insights.append(f\"Outliers detected ({len(outliers)} values outside IQR range)\")\n",
        "\n",
        "    return \"\\n\".join(insights)\n",
        "\n",
        "def plot_top_categorical_counts_with_dynamic_insights(df):\n",
        "    \"\"\"\n",
        "    Plots the top 10 categories based on count.\n",
        "    Uses:\n",
        "    - Pie charts for categorical columns with exactly 3 unique classes.\n",
        "    - Bar charts for other categorical columns.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    filter_numbers=[5, 10, 15, 20]\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "    categorical_columns = [col for col in categorical_columns if not is_likely_id_column(df, col)]\n",
        "\n",
        "    for column in categorical_columns:\n",
        "            category_counts = df[column].value_counts().nlargest(10)\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            if len(df[column].unique()) == 3:  # Use pie chart if exactly 3 unique categories\n",
        "                plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', colors=LIGHT_COLORS)\n",
        "                plt.title(f\"Distribution of {column} (Count)\")\n",
        "                plot_type = \"pie_chart\"\n",
        "                insight_text = generate_dynamic_recommendations(df, column)\n",
        "                plt.figtext(0.5, -0.25, insight_text, wrap=True, horizontalalignment='center', fontsize=10,\n",
        "                    bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3'))\n",
        "                plt.tight_layout()\n",
        "                fig = plt.gcf()\n",
        "                results.append((plot_to_base64(fig), plot_type))\n",
        "    for filter_number in filter_numbers:\n",
        "        for column in categorical_columns:\n",
        "            category_counts = df[column].value_counts().nlargest(filter_number)\n",
        "\n",
        "            plt.figure(figsize=(10, 5))\n",
        "\n",
        "            if len(df[column].unique()) != 3:  # Use bar chart for all other cases\n",
        "                sns.barplot(y=category_counts.index, x=category_counts.values, palette=DARK_COLORS + LIGHT_COLORS)\n",
        "                plt.xlabel(\"Count\")\n",
        "                plt.ylabel(column)\n",
        "                plt.title(f\"Top {filter_number} {column} Categories (by Count)\")\n",
        "                plot_type = \"bar_chart\"\n",
        "                # Generate insights and recommendations\n",
        "                insight_text = generate_dynamic_categorical_insights(df, column)\n",
        "                plt.figtext(0.5, -0.25, insight_text, wrap=True, horizontalalignment='center', fontsize=10,\n",
        "                    bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3'))\n",
        "                plt.tight_layout()\n",
        "                fig = plt.gcf()\n",
        "                results.append((plot_to_base64(fig), plot_type, filter_number))\n",
        "\n",
        "            # plt.show()\n",
        "    return results;\n",
        "\n",
        "def plot_top_numerical_insights(df):\n",
        "    \"\"\"\n",
        "    Analyzes numerical data and visualizes only meaningful distributions.\n",
        "    - Filters out ID-like columns.\n",
        "    - Uses histograms, line plots, and bar charts dynamically.\n",
        "    \"\"\"\n",
        "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    filter_numbers=[5, 10, 15, 20]\n",
        "    # Remove ID-like columns (those with mostly unique values)\n",
        "    filtered_numerical_columns = [col for col in numerical_columns if df[col].nunique() / len(df) < 0.9]\n",
        "    results = []\n",
        "    for filter_number in filter_numbers:\n",
        "        for column in filtered_numerical_columns:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "\n",
        "            # # Use different types of charts based on the data\n",
        "            # if df[column].nunique() > 50:  # Continuous data -> line chart\n",
        "            #     sns.lineplot(data=df[column], color=DARK_COLORS[0])\n",
        "            #     plt.title(f\"Trend of {column} Over Time\")\n",
        "            #     plt.xlabel(\"Index\")\n",
        "            #     plt.ylabel(column)\n",
        "\n",
        "            # if df[column].nunique() > 5 and df[column].nunique() < 50:  # Moderate categories -> bar chart\n",
        "            #     sns.barplot(x=df[column].value_counts().index[:filter_number], y=df[column].value_counts().values[:filter_number], palette=DARK_COLORS + LIGHT_COLORS)\n",
        "            #     plt.title(f\"Top 10 Values of {column}\")\n",
        "            #     plt.xlabel(column)\n",
        "            #     plt.ylabel(\"Count\")\n",
        "            #     plot_type = \"bar_chart\"\n",
        "            # else:  # Discrete numeric values -> histogram\n",
        "            sns.histplot(df[column], kde=True, color=DARK_COLORS[0], bins=filter_number, label=generate_numerical_insights(df, column))\n",
        "            plt.xlabel(column)\n",
        "            plt.ylabel(\"Density\")\n",
        "            plt.title(f\"Distribution of {column}\")\n",
        "            plot_type = \"histogram\"\n",
        "            plt.legend(loc='upper right', fontsize=10, frameon=True, edgecolor='black')\n",
        "            plt.tight_layout()\n",
        "            fig = plt.gcf()\n",
        "            results.append((plot_to_base64(fig), plot_type, filter_number))\n",
        "    return results;\n",
        "        # plt.show()\n",
        "def generate_summary_report_image(df):\n",
        "    \"\"\"\n",
        "    Generates and displays a structured business insights report with enhanced readability,\n",
        "    color-coded insights, and numerical breakdowns.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): A Pandas DataFrame containing categorical data.\n",
        "    \"\"\"\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "    insights = []\n",
        "\n",
        "    # Title Section\n",
        "    insights.append((\"BUSINESS INSIGHTS REPORT\\n\", \"#1A0D26\", \"bold\"))\n",
        "    insights.append((\"This report provides key trends with actionable insights to guide business decisions.\\n\\n\\n\\n\", \"#333333\", \"regular\"))\n",
        "\n",
        "    # Generate insights for each categorical column\n",
        "    for column in categorical_columns:\n",
        "        value_counts = df[column].value_counts()\n",
        "        top_value = value_counts.idxmax()  # Most frequent category\n",
        "        count = value_counts.max()\n",
        "        total = len(df)\n",
        "        percentage = (count / total) * 100\n",
        "\n",
        "        # Define color coding based on percentage\n",
        "        if percentage > 50:\n",
        "            color = \"#B58ED7\"  # Green for dominant categories\n",
        "            action = \"Recommendation: Consider expanding this category as it dominates the market.\"\n",
        "        elif percentage > 20:\n",
        "            color = \"#CEB4E4\"  # Orange for balanced categories\n",
        "            action = \"Recommendation: Maintain and monitor performance trends in this category.\"\n",
        "        else:\n",
        "            color = \"#B22222\"  # Red for underperforming categories\n",
        "            action = \"Recommendation: Analyze why this category has lower engagement and optimize strategies.\"\n",
        "\n",
        "        # Generate formatted insight\n",
        "        insights.append((f\"{column} Analysis\", \"#1A0D26\", \"bold\"))  # Bold column name\n",
        "        insights.append((f\"- Most Frequent Value: {top_value} ({count} occurrences, {percentage:.1f}% of total)\", \"#333333\", \"regular\"))\n",
        "        insights.append((f\"- {action}\\n\", color, \"regular\"))\n",
        "\n",
        "    # Key Actionable Steps\n",
        "    insights.append((\"KEY ACTIONABLE STEPS\\n\", \"#1A0D26\", \"bold\"))\n",
        "    insights.append((\"- Focus on high-performing categories to optimize revenue.\", \"#333333\", \"regular\"))\n",
        "    insights.append((\"- Identify and improve underperforming areas based on trends.\", \"#333333\", \"regular\"))\n",
        "    insights.append((\"- Use data insights to refine marketing, inventory, and logistics strategies.\\n\", \"#333333\", \"regular\"))\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=(12, 8), dpi=100)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_frame_on(False)\n",
        "\n",
        "    # Display text inside the figure with different colors and font weights\n",
        "    y_position = 0.95\n",
        "    for text, color, weight in insights:\n",
        "        wrapped_text = \"\\n\".join(wrap(text, width=90))\n",
        "        ax.text(\n",
        "            0.02, y_position, wrapped_text,\n",
        "            fontsize=14,\n",
        "            va=\"top\", ha=\"left\",\n",
        "            family=\"Times New Roman\",\n",
        "            fontweight=weight,  # Use bold or regular\n",
        "            linespacing=1.5,\n",
        "            color=color\n",
        "        )\n",
        "        y_position -= 0.05  # Adjust spacing\n",
        "    images = []\n",
        "    plot_type='others'\n",
        "    images.append((plot_to_base64(fig), plot_type))\n",
        "    return images\n",
        "\n",
        "    # Show the report image\n",
        "    # plt.show()\n",
        "\n",
        "def is_id_column(df, col_name):\n",
        "    \"\"\"\n",
        "    Detects ID-like columns dynamically based on uniqueness.\n",
        "    If more than 50% of values are unique, the column is likely an identifier.\n",
        "    \"\"\"\n",
        "    return df[col_name].nunique() > (0.5 * len(df))\n",
        "\n",
        "def plot_kde_with_insights(df):\n",
        "    \"\"\"\n",
        "    Generates Kernel Density Estimation (KDE) plots for numerical columns,\n",
        "    filters out ID-like columns, analyzes variance, and annotates insights using a legend.\n",
        "    \"\"\"\n",
        "    numerical_cols = [col for col in df.select_dtypes(include=[np.number]).columns if not is_id_column(df, col)]\n",
        "\n",
        "    if not numerical_cols:\n",
        "        print(\"No suitable numerical columns found for KDE plotting.\")\n",
        "        return\n",
        "    results = []\n",
        "    for col in numerical_cols:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.kdeplot(df[col], shade=True, color=DARK_COLORS[2], linewidth=2, label=f\"Distribution of {col}\")\n",
        "\n",
        "        # Calculate variance and standard deviation\n",
        "        variance = np.var(df[col])\n",
        "        std_dev = np.std(df[col])\n",
        "\n",
        "        # Define variance categories dynamically based on column mean\n",
        "        mean_value = df[col].mean()\n",
        "        low_threshold = mean_value * 0.05\n",
        "        high_threshold = mean_value * 0.5\n",
        "\n",
        "        # Interpret variance levels\n",
        "        if variance > high_threshold:\n",
        "            insight = f\"High Variance: {col} fluctuates significantly.\"\n",
        "            recommendation = f\"Investigate causes (e.g., seasonal trends, outliers).\"\n",
        "            action = f\"Consider segmenting data for clearer patterns.\"\n",
        "        elif variance < low_threshold:\n",
        "            insight = f\"Low Variance: {col} is stable with little change.\"\n",
        "            recommendation = f\"Check for missing diversity in data sources.\"\n",
        "            action = f\"Ensure data reflects realistic variations.\"\n",
        "        else:\n",
        "            insight = f\"Moderate Variance: {col} has some fluctuation.\"\n",
        "            recommendation = f\"Monitor for unusual trends over time.\"\n",
        "            action = f\"Use smoothing techniques if needed.\"\n",
        "\n",
        "        # Plot formatting\n",
        "        plt.title(f\"KDE Plot: {col}\", fontsize=16, fontweight='bold', pad=20)\n",
        "        plt.xlabel(col, fontsize=12)\n",
        "        plt.ylabel(\"Density\", fontsize=12)\n",
        "        plot_type = 'kde'\n",
        "        # Add insights as a legend instead of overlapping text\n",
        "        plt.legend([f\" {insight}\\n {recommendation}\\n {action}\"], loc=\"best\", fontsize=10, frameon=True)\n",
        "        fig = plt.gcf()\n",
        "        results.append((plot_to_base64(fig), plot_type))\n",
        "    return results;\n",
        "        # plt.show()\n",
        "\n",
        "def plot_correlation_matrix(df):\n",
        "    \"\"\"\n",
        "    Plots a correlation matrix with business insights below the graph.\n",
        "    \"\"\"\n",
        "    # Select only numerical columns\n",
        "    numerical_df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "    # Compute correlation matrix\n",
        "    corr_matrix = numerical_df.corr()\n",
        "\n",
        "    # Define color mapping based on correlation strength\n",
        "    cmap = sns.color_palette(DARK_COLORS + LIGHT_COLORS, as_cmap=True)\n",
        "\n",
        "    # Plot the heatmap\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=cmap, linewidths=0.5, vmin=-1, vmax=1)\n",
        "    plt.title(\"Correlation Matrix Analysis\", fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Generate Business Insights\n",
        "    insights = []\n",
        "    for col1 in corr_matrix.columns:\n",
        "        for col2 in corr_matrix.columns:\n",
        "            if col1 != col2:\n",
        "                corr_value = corr_matrix.loc[col1, col2]\n",
        "                if corr_value > 0.7:\n",
        "                    insights.append(f\"{col1} and {col2} have a strong positive correlation ({corr_value:.2f}). This suggests that increasing {col1} will likely increase {col2}.\")\n",
        "                elif corr_value < -0.7:\n",
        "                    insights.append(f\"{col1} and {col2} have a strong negative correlation ({corr_value:.2f}). This means when {col1} increases, {col2} tends to decrease.\")\n",
        "                elif 0.3 < corr_value < 0.7 or -0.7 < corr_value < -0.3:\n",
        "                    insights.append(f\"{col1} and {col2} have a moderate correlation ({corr_value:.2f}). There is a noticeable relationship, but other factors may influence it.\")\n",
        "\n",
        "    # Display Insights Below the Graph\n",
        "    insight_text = \"\\n\".join(insights)\n",
        "    plt.figtext(0.5, -0.3, insight_text, wrap=True, horizontalalignment='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
        "    results = []\n",
        "    fig = plt.gcf()\n",
        "    plot_type = 'correlation'\n",
        "    results.append((plot_to_base64(fig), plot_type));\n",
        "    return results;\n",
        "    # plt.show()\n",
        "\n",
        "def forecast_business_metrics(df):\n",
        "    # Keywords to identify relevant columns\n",
        "    keywords = [\n",
        "        \"sales\", \"profit\", \"revenue\", \"income\", \"return\", \"proceeds\", \"earnings\",\n",
        "        \"yield\", \"incoming\", \"gain\", \"transactions\", \"deals\", \"purchases\",\n",
        "        \"auctions\", \"bargains\", \"trades\", \"buys\", \"negotiations\"\n",
        "    ]\n",
        "\n",
        "    # Identify columns dynamically\n",
        "    target_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in keywords)]\n",
        "\n",
        "    if not target_cols:\n",
        "        print(\"No relevant financial columns found. Forecasting is skipped.\")\n",
        "        return\n",
        "\n",
        "    # Create a Date column from Year, Month, Day\n",
        "    df['Date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
        "\n",
        "    # Aggregate data monthly\n",
        "    df = df.groupby(pd.Grouper(key='Date', freq='M'))[target_cols].sum().reset_index()\n",
        "    images = []\n",
        "    filter_numbers = [6, 9, 12, 18, 24, 36]\n",
        "    for filter_number in filter_numbers:\n",
        "        for col in target_cols:\n",
        "            plt.figure(figsize=(12, 6))  # Separate figure for each metric\n",
        "\n",
        "            # Prepare data for Prophet\n",
        "            data = df[['Date', col]].rename(columns={'Date': 'ds', col: 'y'})\n",
        "\n",
        "            # Fit the Prophet model\n",
        "            model = Prophet()\n",
        "            model.fit(data)\n",
        "\n",
        "            # Create future dates\n",
        "            future = model.make_future_dataframe(periods=filter_number, freq='M')\n",
        "            forecast = model.predict(future)\n",
        "\n",
        "            # Calculate forecast accuracy\n",
        "            actual_values = data['y'].values\n",
        "            predicted_values = model.predict(data)['yhat'].values\n",
        "            mape = mean_absolute_percentage_error(actual_values, predicted_values) * 100\n",
        "            accuracy = 100 - mape  # Accuracy is 100 - MAPE\n",
        "\n",
        "            # Plot results\n",
        "            plt.plot(data['ds'], data['y'], label=f\"Historical {col}\", color=\"#B58ED7\")\n",
        "            plt.plot(forecast['ds'], forecast['yhat'], label=f\"Forecast {col} (Accuracy: {accuracy:.2f}%)\", color=\"#693696\" , linestyle= 'dotted')\n",
        "\n",
        "            plt.title(f\"{col} Forecast\")\n",
        "            plt.xlabel(\"Date\")\n",
        "            plt.ylabel(\"Value\")\n",
        "            plt.legend(loc=\"best\")\n",
        "            fig = plt.gcf()\n",
        "            plot_type='forecast'\n",
        "            images.append((plot_to_base64(fig), plot_type, filter_number))\n",
        "            # plt.show()\n",
        "    return images;\n",
        "\n",
        "@app.exception_handler(Exception)\n",
        "async def global_exception_handler(request, exc):\n",
        "    return JSONResponse(\n",
        "        status_code=500,\n",
        "        content={\n",
        "            \"error\": str(exc),\n",
        "            \"detail\": traceback.format_exc()\n",
        "        }\n",
        "    )\n",
        "class CsvRequest(BaseModel):\n",
        "    cloudinary_url: str\n",
        "\n",
        "\n",
        "@app.post(\"/clean-data\")\n",
        "async def clean_data(csv_request: CsvRequest):\n",
        "    try:\n",
        "        print(f\"Received Cloudinary URL: {csv_request.cloudinary_url}\")\n",
        "        async with httpx.AsyncClient() as client:\n",
        "            response = await client.get(csv_request.cloudinary_url)\n",
        "            if response.status_code != 200:\n",
        "                raise HTTPException(\n",
        "                    status_code=400,\n",
        "                    detail=\"Failed to download CSV from Cloudinary URL\"\n",
        "                )\n",
        "        content = response.content\n",
        "        print(f\"Downloaded content length: {len(content)} bytes\")\n",
        "\n",
        "        # Try different encodings\n",
        "        encodings_to_try = ['latin-1', 'utf-8', 'iso-8859-1', 'cp1252']\n",
        "        data = None\n",
        "\n",
        "        for encoding in encodings_to_try:\n",
        "            try:\n",
        "                print(f\"Trying {encoding} encoding...\")\n",
        "                data = pd.read_csv(io.StringIO(content.decode(encoding)))\n",
        "                print(f\"Successfully read CSV with {encoding} encoding\")\n",
        "                break\n",
        "            except UnicodeDecodeError:\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {encoding}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if data is None:\n",
        "            raise HTTPException(\n",
        "                status_code=400,\n",
        "                detail=\"Could not read CSV file with any supported encoding\"\n",
        "            )\n",
        "\n",
        "        # Step 2: Apply AutoClean\n",
        "        cleaned_data = AutoClean(\n",
        "            data,\n",
        "            mode = 'manual',\n",
        "            missing_num=\"auto\",  # Impute missing numerical data\n",
        "            missing_categ='auto',  # Impute missing categorical data\n",
        "            outliers='auto',  # Detect and handle outliers\n",
        "            duplicates = 'auto',\n",
        "            extract_datetime = 's',\n",
        "        )\n",
        "\n",
        "\n",
        "        df_cleaned = cleaned_data.output\n",
        "        # csv_buffer = io.StringIO()\n",
        "        # df_cleaned.to_csv(csv_buffer, index=False)  # Save DataFrame to CSV format\n",
        "        # csv_buffer.seek(0)  # Reset buffer position\n",
        "\n",
        "        # 5️⃣ Upload the cleaned CSV to Cloudinary\n",
        "        # /// / / upload_result = cloudinary.uploader.upload_large(csv_buffer, resource_type=\"raw\", folder=\"processed_csvs\")\n",
        "        # upload_result = cloudinary.uploader.upload(csv_buffer.getvalue(), resource_type=\"raw\", folder=\"processed_csvs\")\n",
        "\n",
        "        # Convert DataFrame to CSV string and encode it to bytes\n",
        "        csv_str = df_cleaned.to_csv(index=False, sep=',', encoding='utf-8-sig', date_format='%Y-%m-%d')\n",
        "        csv_bytes = csv_str.encode('utf-8-sig')\n",
        "        file_size = len(csv_bytes)  # Get file size in bytes\n",
        "\n",
        "        # Wrap the CSV bytes in a BytesIO stream (so it's not misinterpreted as a file name)\n",
        "        csv_buffer = io.BytesIO(csv_bytes)\n",
        "        csv_buffer.seek(0)\n",
        "\n",
        "        # Define a threshold in bytes (e.g., 10 MB)\n",
        "        THRESHOLD = 10 * 1024 * 1024  # 10 MB\n",
        "\n",
        "        upload_result = None  # Initialize variable to ensure scope\n",
        "\n",
        "        if file_size < THRESHOLD:\n",
        "            # For smaller files, use the standard upload method with the file-like object\n",
        "            upload_result = cloudinary.uploader.upload(\n",
        "                csv_buffer,\n",
        "                resource_type=\"raw\",\n",
        "                folder=\"processed_csvs\"\n",
        "            )\n",
        "        else:\n",
        "            # For larger files, use upload_large with the same binary stream\n",
        "            csv_buffer.seek(0)  # Ensure pointer is at the beginning\n",
        "            upload_result = cloudinary.uploader.upload_large(\n",
        "                csv_buffer,\n",
        "                resource_type=\"raw\",\n",
        "                folder=\"processed_csvs\"\n",
        "            )\n",
        "\n",
        "        return {\n",
        "            \"message\": \"Cleaning completed successfully\",\n",
        "            'cleaned_csv': upload_result[\"secure_url\"]\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {str(e)}\")\n",
        "        print(f\"Traceback: {traceback.format_exc()}\")\n",
        "        raise HTTPException(\n",
        "            status_code=500,\n",
        "            detail={\n",
        "                \"error\": str(e),\n",
        "                \"traceback\": traceback.format_exc()\n",
        "            }\n",
        "    )\n",
        "\n",
        "@app.post(\"/analyze-data\")\n",
        "async def analyze_data(csv_request: CsvRequest):\n",
        "    try:\n",
        "        print(f\"Received Cloudinary URL: {csv_request.cloudinary_url}\")\n",
        "        async with httpx.AsyncClient() as client:\n",
        "            response = await client.get(csv_request.cloudinary_url)\n",
        "            if response.status_code != 200:\n",
        "                raise HTTPException(\n",
        "                    status_code=400,\n",
        "                    detail=\"Failed to download CSV from Cloudinary URL\"\n",
        "                )\n",
        "        content = response.content\n",
        "        print(f\"Downloaded content length: {len(content)} bytes\")\n",
        "\n",
        "        # Try different encodings\n",
        "        encodings_to_try = ['latin-1', 'utf-8', 'iso-8859-1', 'cp1252']\n",
        "        data = None\n",
        "\n",
        "        for encoding in encodings_to_try:\n",
        "            try:\n",
        "                print(f\"Trying {encoding} encoding...\")\n",
        "                data = pd.read_csv(io.StringIO(content.decode(encoding)))\n",
        "                print(f\"Successfully read CSV with {encoding} encoding\")\n",
        "                break\n",
        "            except UnicodeDecodeError:\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {encoding}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if data is None:\n",
        "            raise HTTPException(\n",
        "                status_code=400,\n",
        "                detail=\"Could not read CSV file with any supported encoding\"\n",
        "            )\n",
        "        cleaned_data = AutoClean(\n",
        "            data,\n",
        "            mode = 'manual',\n",
        "            missing_num=\"auto\",  # Impute missing numerical data\n",
        "            missing_categ='auto',  # Impute missing categorical data\n",
        "            outliers='auto',  # Detect and handle outliers\n",
        "            duplicates = 'auto',\n",
        "            extract_datetime = 's')\n",
        "        # Generate visualizations\n",
        "        images = []\n",
        "        # Step 1: Initial exploration\n",
        "        # print(\"Initial Dataset Overview:\\n\")\n",
        "        # print(\"Null values in each column:\\n\", data.isnull().sum())\n",
        "        # print(\"\\nTotal Duplicates:\", data.duplicated().sum())\n",
        "\n",
        "        # data.head()\n",
        "        # print no of rows\n",
        "        # print(\"Number of rows in the dataset:\", len(data))\n",
        "        # print the outliers\n",
        "        # data.describe()\n",
        "        # identify column types\n",
        "        # data.dtypes\n",
        "        df_cleaned = cleaned_data.output\n",
        "################################################################################################################################\n",
        "        # cleaned csv phase (cloudinary)\n",
        "        csv_str = df_cleaned.to_csv(index=False)\n",
        "        csv_bytes = csv_str.encode('utf-8')\n",
        "        file_size = len(csv_bytes)  # Get file size in bytes\n",
        "\n",
        "        # Wrap the CSV bytes in a BytesIO stream (so it's not misinterpreted as a file name)\n",
        "        csv_buffer = io.BytesIO(csv_bytes)\n",
        "        csv_buffer.seek(0)\n",
        "\n",
        "        # Define a threshold in bytes (e.g., 10 MB)\n",
        "        THRESHOLD = 10 * 1024 * 1024  # 10 MB\n",
        "\n",
        "        upload_result = None  # Initialize variable to ensure scope\n",
        "\n",
        "        if file_size < THRESHOLD:\n",
        "            # For smaller files, use the standard upload method with the file-like object\n",
        "            upload_result = cloudinary.uploader.upload(\n",
        "                csv_buffer,\n",
        "                resource_type=\"raw\",\n",
        "                folder=\"processed_csvs\"\n",
        "            )\n",
        "        else:\n",
        "            # For larger files, use upload_large with the same binary stream\n",
        "            csv_buffer.seek(0)  # Ensure pointer is at the beginning\n",
        "            upload_result = cloudinary.uploader.upload_large(\n",
        "                csv_buffer,\n",
        "                resource_type=\"raw\",\n",
        "                folder=\"processed_csvs\"\n",
        "            )\n",
        "################################################################################################################################################\n",
        "        # data generation\n",
        "        # Define color palettes\n",
        "        DARK_COLORS = [\"#1A0D26\", \"#351B4B\", \"#4F2871\", \"#693696\"]\n",
        "        LIGHT_COLORS = [\"#9C69C9\", \"#B58ED7\", \"#CEB4E4\", \"#E6D9F2\"]\n",
        "        images = []\n",
        "        images.extend(plot_top_categorical_counts_with_dynamic_insights(df_cleaned))\n",
        "        images.extend(plot_top_numerical_insights(df_cleaned))\n",
        "        images.extend(generate_summary_report_image(df_cleaned))\n",
        "        kde_images = plot_kde_with_insights(df_cleaned)\n",
        "        if kde_images:\n",
        "          images.extend(kde_images)\n",
        "        images.extend(plot_correlation_matrix(df_cleaned))\n",
        "        forecast_images = forecast_business_metrics(df_cleaned)\n",
        "        if forecast_images:\n",
        "            images.extend(forecast_images)\n",
        "        return {\n",
        "            \"message\": \"Analysis completed successfully\",\n",
        "            \"images\": images,\n",
        "            \"cleaned_csv\": upload_result[\"secure_url\"]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {str(e)}\")\n",
        "        print(f\"Traceback: {traceback.format_exc()}\")\n",
        "        raise HTTPException(\n",
        "            status_code=500,\n",
        "            detail={\n",
        "                \"error\": str(e),\n",
        "                \"traceback\": traceback.format_exc()\n",
        "            }\n",
        "    )\n",
        "\n",
        "# Setup ngrok\n",
        "ngrok.set_auth_token(\"2swgwcEJ5hsXEst7a5WBLtv58s8_5FtZDTirtSBKrSL4e8HUR\")  # Replace with your ngrok auth token\n",
        "\n",
        "# Run the FastAPI app\n",
        "if __name__ == \"__main__\":\n",
        "  public_url = ngrok.connect(8000, bind_tls=True).public_url\n",
        "  print(f\"FastAPI is publicly accessible at: {public_url}\")\n",
        "  uvicorn.run(\n",
        "      \"__main__:app\",\n",
        "      host=\"0.0.0.0\",\n",
        "      port=8000,\n",
        "      log_config=None,  # Disable Uvicorn's default logging\n",
        "      access_log=False  # Disable access logs\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb-L0q0zB0Z7",
        "outputId": "5f162a2d-278b-4afb-f42a-0ccb58664f34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.11)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: pycaret in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py-AutoClean in /usr/local/lib/python3.11/dist-packages (1.1.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (0.0.20)\n",
            "Requirement already satisfied: cloudinary in /usr/local/lib/python3.11/dist-packages (1.42.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: ipython>=5.5.0 in /usr/local/lib/python3.11/dist-packages (from pycaret) (7.34.0)\n",
            "Requirement already satisfied: ipywidgets>=7.6.5 in /usr/local/lib/python3.11/dist-packages (from pycaret) (7.7.1)\n",
            "Requirement already satisfied: tqdm>=4.62.0 in /usr/local/lib/python3.11/dist-packages (from pycaret) (4.67.1)\n",
            "Requirement already satisfied: numpy<1.27,>=1.21 in /usr/local/lib/python3.11/dist-packages (from pycaret) (1.26.4)\n",
            "Requirement already satisfied: jinja2>=3 in /usr/local/lib/python3.11/dist-packages (from pycaret) (3.1.6)\n",
            "Requirement already satisfied: scipy<=1.11.4,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from pycaret) (1.11.4)\n",
            "Requirement already satisfied: joblib<1.4,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pycaret) (1.3.2)\n",
            "Requirement already satisfied: scikit-learn>1.4.0 in /usr/local/lib/python3.11/dist-packages (from pycaret) (1.4.2)\n",
            "Requirement already satisfied: pyod>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from pycaret) (2.0.3)\n",
            "Requirement already satisfied: imbalanced-learn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from pycaret) (0.13.0)\n",
            "Requirement already satisfied: category-encoders>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pycaret) (2.7.0)\n",
            "Requirement already satisfied: lightgbm>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from pycaret) (4.5.0)\n",
            "Requirement already satisfied: numba>=0.55.0 in /usr/local/lib/python3.11/dist-packages (from pycaret) (0.60.0)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from pycaret) (2.32.3)\n",
            "Requirement already satisfied: psutil>=5.9.0 in /usr/local/lib/python3.11/dist-packages (from pycaret) (5.9.5)\n",
            "Requirement already satisfied: markupsafe>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from pycaret) (3.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.12.0 in /usr/local/lib/python3.11/dist-packages (from pycaret) (8.6.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pycaret) (5.10.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from pycaret) (3.1.1)\n",
            "Requirement already satisfied: deprecation>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycaret) (2.1.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from pycaret) (3.5.0)\n",
            "Requirement already satisfied: scikit-plot>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from pycaret) (0.3.7)\n",
            "Requirement already satisfied: yellowbrick>=1.4 in /usr/local/lib/python3.11/dist-packages (from pycaret) (1.5)\n",
            "Requirement already satisfied: plotly>=5.14.0 in /usr/local/lib/python3.11/dist-packages (from pycaret) (5.24.1)\n",
            "Requirement already satisfied: kaleido>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from pycaret) (0.2.1)\n",
            "Requirement already satisfied: schemdraw==0.15 in /usr/local/lib/python3.11/dist-packages (from pycaret) (0.15)\n",
            "Requirement already satisfied: plotly-resampler>=0.8.3.1 in /usr/local/lib/python3.11/dist-packages (from pycaret) (0.10.0)\n",
            "Requirement already satisfied: statsmodels>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pycaret) (0.14.4)\n",
            "Requirement already satisfied: sktime==0.26.0 in /usr/local/lib/python3.11/dist-packages (from pycaret) (0.26.0)\n",
            "Requirement already satisfied: tbats>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from pycaret) (1.1.3)\n",
            "Requirement already satisfied: pmdarima>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from pycaret) (2.0.4)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.11/dist-packages (from pycaret) (3.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from sktime==0.26.0->pycaret) (24.2)\n",
            "Requirement already satisfied: scikit-base<0.8.0 in /usr/local/lib/python3.11/dist-packages (from sktime==0.26.0->pycaret) (0.7.8)\n",
            "Requirement already satisfied: loguru in /usr/local/lib/python3.11/dist-packages (from py-AutoClean) (0.7.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from cloudinary) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.11/dist-packages (from cloudinary) (2.3.0)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category-encoders>=2.4.0->pycaret) (1.0.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn>=0.12.0->pycaret) (0.1.3)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn>=0.12.0->pycaret) (3.5.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.12.0->pycaret) (3.21.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->pycaret) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->pycaret) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->pycaret) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->pycaret) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->pycaret) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->pycaret) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->pycaret) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->pycaret) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->pycaret) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.5.0->pycaret) (4.9.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=7.6.5->pycaret) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=7.6.5->pycaret) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=7.6.5->pycaret) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=7.6.5->pycaret) (3.0.13)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=4.2.0->pycaret) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=4.2.0->pycaret) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat>=4.2.0->pycaret) (5.7.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.55.0->pycaret) (0.43.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.14.0->pycaret) (9.0.0)\n",
            "Requirement already satisfied: dash>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from plotly-resampler>=0.8.3.1->pycaret) (2.18.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from plotly-resampler>=0.8.3.1->pycaret) (3.10.15)\n",
            "Requirement already satisfied: tsdownsample>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from plotly-resampler>=0.8.3.1->pycaret) (0.1.4.1)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.11/dist-packages (from pmdarima>=2.0.4->pycaret) (3.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->pycaret) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx) (1.3.1)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (3.0.3)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.11/dist-packages (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (3.0.6)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.11/dist-packages (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (2.0.0)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.11/dist-packages (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (2.0.0)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (5.0.0)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.11/dist-packages (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (1.3.4)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (6.1.12)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (6.4.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.5.0->pycaret) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (0.23.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=4.2.0->pycaret) (4.3.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.5.0->pycaret) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.5.0->pycaret) (0.2.13)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (6.5.5)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (1.9.0)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (23.1.0)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.2.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (3.1.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.4.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (2.22)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok fastapi uvicorn nest_asyncio httpx py-AutoClean pandas matplotlib seaborn python-multipart cloudinary\n",
        "import httpx\n",
        "import csv\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "import pandas as pd\n",
        "from AutoClean import AutoClean\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import io\n",
        "from fastapi.responses import StreamingResponse, JSONResponse\n",
        "import base64\n",
        "from typing import List, Dict\n",
        "from textwrap import wrap\n",
        "import traceback\n",
        "import numpy as np\n",
        "from pydantic import BaseModel\n",
        "import os\n",
        "import logging\n",
        "import cloudinary\n",
        "import cloudinary.uploader\n",
        "import cloudinary.api\n",
        "from prophet import Prophet\n",
        "from sklearn.metrics import mean_absolute_percentage_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NaH4OHhs8wRG",
        "outputId": "c83c6a59-f6cb-4d53-ba55-fa5a2f8fce2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI is publicly accessible at: https://393b-107-167-183-9.ngrok-free.app\n",
            "Received Cloudinary URL: https://res.cloudinary.com/dwd6kau8a/raw/upload/v1741876656/fy8ptfjyv1hb3zliuz1b.csv\n",
            "Downloaded content length: 2305990 bytes\n",
            "Trying latin-1 encoding...\n",
            "Successfully read CSV with latin-1 encoding\n",
            "AutoClean process completed in 3.895565 seconds\n",
            "Logfile saved to: /content/autoclean.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/h3lbljgn.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/2r5__boi.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=38431', 'data', 'file=/tmp/tmph3gyfuo2/h3lbljgn.json', 'init=/tmp/tmph3gyfuo2/2r5__boi.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_model7hi91kpc/prophet_model-20250313143920.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:20 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:20 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/p05dc036.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/nyh98arx.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=40279', 'data', 'file=/tmp/tmph3gyfuo2/p05dc036.json', 'init=/tmp/tmph3gyfuo2/nyh98arx.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_modellw4r2rfl/prophet_model-20250313143921.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:21 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:21 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/spbotuic.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/x4yqyo7t.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=84967', 'data', 'file=/tmp/tmph3gyfuo2/spbotuic.json', 'init=/tmp/tmph3gyfuo2/x4yqyo7t.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_modelrcarmqx0/prophet_model-20250313143922.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:22 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:22 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/b4qbdgk2.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/3h70l7st.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=72114', 'data', 'file=/tmp/tmph3gyfuo2/b4qbdgk2.json', 'init=/tmp/tmph3gyfuo2/3h70l7st.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_modelx7kjwnf7/prophet_model-20250313143922.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:22 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:23 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/dxhwxc03.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/zmiwmu01.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=83840', 'data', 'file=/tmp/tmph3gyfuo2/dxhwxc03.json', 'init=/tmp/tmph3gyfuo2/zmiwmu01.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_model4w66r6p6/prophet_model-20250313143923.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:23 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:24 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/nlb524rx.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/6dwkiyux.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=50643', 'data', 'file=/tmp/tmph3gyfuo2/nlb524rx.json', 'init=/tmp/tmph3gyfuo2/6dwkiyux.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_modelbla1uxnx/prophet_model-20250313143924.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:24 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:24 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/fvb6fp7z.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/63jky0nu.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=69261', 'data', 'file=/tmp/tmph3gyfuo2/fvb6fp7z.json', 'init=/tmp/tmph3gyfuo2/63jky0nu.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_model8lws3wtb/prophet_model-20250313143925.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:25 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:25 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/4yk49cms.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/eew8gcc0.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=37338', 'data', 'file=/tmp/tmph3gyfuo2/4yk49cms.json', 'init=/tmp/tmph3gyfuo2/eew8gcc0.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_model6qrgcfy9/prophet_model-20250313143926.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:26 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:26 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/xzbtuc49.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/n3do8om6.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=73959', 'data', 'file=/tmp/tmph3gyfuo2/xzbtuc49.json', 'init=/tmp/tmph3gyfuo2/n3do8om6.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_modeltv5pri5b/prophet_model-20250313143926.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:26 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:27 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/ig93w76y.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/xff_8n2z.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=4720', 'data', 'file=/tmp/tmph3gyfuo2/ig93w76y.json', 'init=/tmp/tmph3gyfuo2/xff_8n2z.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_modelbsah2cau/prophet_model-20250313143927.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:27 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:28 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/pr_7pmch.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/caqtnm4a.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=89071', 'data', 'file=/tmp/tmph3gyfuo2/pr_7pmch.json', 'init=/tmp/tmph3gyfuo2/caqtnm4a.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_modelrkwbdfay/prophet_model-20250313143928.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:28 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:28 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/tta7er1r.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/zw4740jy.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=94586', 'data', 'file=/tmp/tmph3gyfuo2/tta7er1r.json', 'init=/tmp/tmph3gyfuo2/zw4740jy.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_model_97o5bl0/prophet_model-20250313143929.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:29 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:30 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "DARK_COLORS = [\"#1A0D26\", \"#351B4B\", \"#4F2871\", \"#693696\"]\n",
        "LIGHT_COLORS = [\"#9C69C9\", \"#B58ED7\", \"#CEB4E4\", \"#E6D9F2\"]\n",
        "\n",
        "# Apply nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "cloudinary.config(\n",
        "    cloud_name=\"dwd6kau8a\",\n",
        "    api_key=\"414118375842875\",\n",
        "    api_secret=\"99IAqTayxvBkd2aC5DVY1kj1jR0\"\n",
        ")\n",
        "\n",
        "# CORS middleware configuration\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "def plot_to_base64(fig):\n",
        "    \"\"\"Convert matplotlib plot to base64 string\"\"\"\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format='png', bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "    plt.close(fig)\n",
        "    return image_base64\n",
        "\n",
        "def generate_dynamic_insights(df, column):\n",
        "    \"\"\"\n",
        "    Generates automated business insights based on categorical distributions.\n",
        "    \"\"\"\n",
        "    unique_values = df[column].nunique()\n",
        "    total_values = len(df[column])\n",
        "    top_category = df[column].value_counts().idxmax()\n",
        "    top_category_percentage = df[column].value_counts(normalize=True).iloc[0]\n",
        "\n",
        "    # Determine insights based on data patterns\n",
        "    insights = []\n",
        "\n",
        "    if unique_values > total_values * 0.5:\n",
        "        insights.append(f\"{column} appears to be a unique identifier. It is not useful for aggregation but can be used for tracking trends like customer retention and fraud detection.\")\n",
        "\n",
        "    if top_category_percentage > 0.5:\n",
        "        insights.append(f\"{column} is dominated by one category ({top_category}, {top_category_percentage:.1%}). Consider diversifying products or marketing efforts to create a balanced market.\")\n",
        "\n",
        "    if unique_values > 50 and top_category_percentage < 0.05:\n",
        "        insights.append(f\"{column} has a high number of unique categories with no clear dominance. Clustering techniques (e.g., K-Means) can help identify meaningful groups.\")\n",
        "\n",
        "    if unique_values > 5 and top_category_percentage < 0.3:\n",
        "        insights.append(f\"{column} has a well-distributed set of categories. Investigate potential correlations between {column} and key business metrics like revenue or sales.\")\n",
        "\n",
        "    if (df[column].value_counts(normalize=True) < 0.01).sum() > unique_values * 0.5:\n",
        "        insights.append(f\"{column} contains many low-frequency categories. Identify if these represent niche products, seasonal trends, or potential data errors.\")\n",
        "\n",
        "    if unique_values < total_values * 0.05:\n",
        "        insights.append(f\"{column} likely represents key business attributes such as product types or customer segments. Use this to optimize inventory and targeted marketing.\")\n",
        "\n",
        "    return \"\\n\".join(insights) if insights else f\"{column} contains meaningful business insights. Further analysis can help optimize business strategies.\"\n",
        "\n",
        "def generate_numerical_insights(df, column):\n",
        "    \"\"\"Generates insights based on numerical distributions, skewness, and outliers.\"\"\"\n",
        "    insights = []\n",
        "    mean_val = df[column].mean()\n",
        "    median_val = df[column].median()\n",
        "    std_dev = df[column].std()\n",
        "    skewness = df[column].skew()\n",
        "    kurtosis = df[column].kurtosis()\n",
        "\n",
        "    insights.append(f\"Mean: {mean_val:.2f}, Median: {median_val:.2f}, Std Dev: {std_dev:.2f}\")\n",
        "\n",
        "    if abs(skewness) > 1:\n",
        "        insights.append(f\"Highly skewed (Skewness: {skewness:.2f})\")\n",
        "    elif abs(skewness) > 0.5:\n",
        "        insights.append(f\"Moderately skewed (Skewness: {skewness:.2f})\")\n",
        "    else:\n",
        "        insights.append(f\"Approximately Normal distribution (Skewness: {skewness:.2f})\")\n",
        "\n",
        "    if kurtosis > 3:\n",
        "        insights.append(f\"High peak and heavy tails (Kurtosis: {kurtosis:.2f})\")\n",
        "    elif kurtosis < -1:\n",
        "        insights.append(f\"Flatter distribution (Kurtosis: {kurtosis:.2f})\")\n",
        "\n",
        "    q1, q3 = df[column].quantile([0.25, 0.75])\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column]\n",
        "\n",
        "    if not outliers.empty:\n",
        "        insights.append(f\"Outliers detected ({len(outliers)} values outside IQR range)\")\n",
        "\n",
        "    return \"\\n\".join(insights)\n",
        "\n",
        "\n",
        "def plot_top_categorical_counts_with_dynamic_insights(df):\n",
        "    \"\"\"\n",
        "    Plots the top 10 categories based on count.\n",
        "    Uses:\n",
        "    - Pie charts for categorical columns with exactly 3 unique classes.\n",
        "    - Bar charts for other categorical columns.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    filter_numbers=[5, 10, 15, 20]\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "    for column in categorical_columns:\n",
        "            category_counts = df[column].value_counts().nlargest(10)\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            if len(df[column].unique()) == 3:  # Use pie chart if exactly 3 unique categories\n",
        "                plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', colors=LIGHT_COLORS)\n",
        "                plt.title(f\"Distribution of {column} (Count)\")\n",
        "                plot_type = \"pie_chart\"\n",
        "                insight_text = generate_dynamic_insights(df, column)\n",
        "                plt.figtext(0.5, -0.25, insight_text, wrap=True, horizontalalignment='center', fontsize=10, bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3'))\n",
        "                plt.tight_layout()\n",
        "                fig = plt.gcf()\n",
        "                results.append((plot_to_base64(fig), plot_type))\n",
        "    for filter_number in filter_numbers:\n",
        "        for column in categorical_columns:\n",
        "            category_counts = df[column].value_counts().nlargest(filter_number)\n",
        "\n",
        "            plt.figure(figsize=(10, 5))\n",
        "\n",
        "            if len(df[column].unique()) != 3:  # Use bar chart for all other cases\n",
        "                sns.barplot(y=category_counts.index, x=category_counts.values, palette=DARK_COLORS + LIGHT_COLORS)\n",
        "                plt.xlabel(\"Count\")\n",
        "                plt.ylabel(column)\n",
        "                plt.title(f\"Top {filter_number} {column} Categories (by Count)\")\n",
        "                plot_type = \"bar_chart\"\n",
        "                # Generate insights and recommendations\n",
        "                insight_text = generate_dynamic_insights(df, column)\n",
        "                plt.figtext(0.5, -0.25, insight_text, wrap=True, horizontalalignment='center', fontsize=10, bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3'))\n",
        "                plt.tight_layout()\n",
        "                fig = plt.gcf()\n",
        "                results.append((plot_to_base64(fig), plot_type, filter_number))\n",
        "\n",
        "            # plt.show()\n",
        "    return results;\n",
        "\n",
        "def plot_top_numerical_insights(df):\n",
        "    \"\"\"\n",
        "    Analyzes numerical data and visualizes only meaningful distributions.\n",
        "    - Filters out ID-like columns.\n",
        "    - Uses histograms, line plots, and bar charts dynamically.\n",
        "    \"\"\"\n",
        "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    filter_numbers=[5, 10, 15, 20]\n",
        "    # Remove ID-like columns (those with mostly unique values)\n",
        "    filtered_numerical_columns = [col for col in numerical_columns if df[col].nunique() / len(df) < 0.9]\n",
        "    results = []\n",
        "    for filter_number in filter_numbers:\n",
        "        for column in filtered_numerical_columns:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "\n",
        "            # # Use different types of charts based on the data\n",
        "            # if df[column].nunique() > 50:  # Continuous data -> line chart\n",
        "            #     sns.lineplot(data=df[column], color=DARK_COLORS[0])\n",
        "            #     plt.title(f\"Trend of {column} Over Time\")\n",
        "            #     plt.xlabel(\"Index\")\n",
        "            #     plt.ylabel(column)\n",
        "\n",
        "            if df[column].nunique() > 5 and df[column].nunique() < 50:  # Moderate categories -> bar chart\n",
        "                sns.barplot(x=df[column].value_counts().index[:filter_number], y=df[column].value_counts().values[:filter_number], palette=DARK_COLORS + LIGHT_COLORS)\n",
        "                plt.title(f\"Top 10 Values of {column}\")\n",
        "                plt.xlabel(column)\n",
        "                plt.ylabel(\"Count\")\n",
        "                plot_type = \"bar_chart\"\n",
        "            else:  # Discrete numeric values -> histogram\n",
        "                sns.histplot(df[column], bins=filter_number, kde=True, color=DARK_COLORS[1])\n",
        "                plt.title(f\"Distribution of {column}\")\n",
        "                plt.xlabel(column)\n",
        "                plt.ylabel(\"Frequency\")\n",
        "                plot_type = \"histogram\"\n",
        "\n",
        "            # Insights and recommendations\n",
        "            plt.figtext(0.5, -0.25, f\"Insight: {column} shows meaningful business trends.\\nRecommendation: Use this to predict future sales, demand, or operational efficiency.\",\n",
        "                    wrap=True, horizontalalignment='center', fontsize=10, bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3'))\n",
        "\n",
        "            plt.tight_layout()\n",
        "            fig = plt.gcf()\n",
        "            results.append((plot_to_base64(fig), plot_type, filter_number))\n",
        "    return results;\n",
        "        # plt.show()\n",
        "def generate_summary_report_image(df):\n",
        "    \"\"\"\n",
        "    Generates and displays a structured business insights report with enhanced readability,\n",
        "    color-coded insights, and numerical breakdowns.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): A Pandas DataFrame containing categorical data.\n",
        "    \"\"\"\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "    insights = []\n",
        "\n",
        "    # Title Section\n",
        "    insights.append((\"BUSINESS INSIGHTS REPORT\\n\", \"#1A0D26\", \"bold\"))\n",
        "    insights.append((\"This report provides key trends with actionable insights to guide business decisions.\\n\\n\\n\\n\", \"#333333\", \"regular\"))\n",
        "\n",
        "    # Generate insights for each categorical column\n",
        "    for column in categorical_columns:\n",
        "        value_counts = df[column].value_counts()\n",
        "        top_value = value_counts.idxmax()  # Most frequent category\n",
        "        count = value_counts.max()\n",
        "        total = len(df)\n",
        "        percentage = (count / total) * 100\n",
        "\n",
        "        # Define color coding based on percentage\n",
        "        if percentage > 50:\n",
        "            color = \"#B58ED7\"  # Green for dominant categories\n",
        "            action = \"Recommendation: Consider expanding this category as it dominates the market.\"\n",
        "        elif percentage > 20:\n",
        "            color = \"#CEB4E4\"  # Orange for balanced categories\n",
        "            action = \"Recommendation: Maintain and monitor performance trends in this category.\"\n",
        "        else:\n",
        "            color = \"#B22222\"  # Red for underperforming categories\n",
        "            action = \"Recommendation: Analyze why this category has lower engagement and optimize strategies.\"\n",
        "\n",
        "        # Generate formatted insight\n",
        "        insights.append((f\"{column} Analysis\", \"#1A0D26\", \"bold\"))  # Bold column name\n",
        "        insights.append((f\"- Most Frequent Value: {top_value} ({count} occurrences, {percentage:.1f}% of total)\", \"#333333\", \"regular\"))\n",
        "        insights.append((f\"- {action}\\n\", color, \"regular\"))\n",
        "\n",
        "    # Key Actionable Steps\n",
        "    insights.append((\"KEY ACTIONABLE STEPS\\n\", \"#1A0D26\", \"bold\"))\n",
        "    insights.append((\"- Focus on high-performing categories to optimize revenue.\", \"#333333\", \"regular\"))\n",
        "    insights.append((\"- Identify and improve underperforming areas based on trends.\", \"#333333\", \"regular\"))\n",
        "    insights.append((\"- Use data insights to refine marketing, inventory, and logistics strategies.\\n\", \"#333333\", \"regular\"))\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=(12, 8), dpi=100)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_frame_on(False)\n",
        "\n",
        "    # Display text inside the figure with different colors and font weights\n",
        "    y_position = 0.95\n",
        "    for text, color, weight in insights:\n",
        "        wrapped_text = \"\\n\".join(wrap(text, width=90))\n",
        "        ax.text(\n",
        "            0.02, y_position, wrapped_text,\n",
        "            fontsize=14,\n",
        "            va=\"top\", ha=\"left\",\n",
        "            family=\"Times New Roman\",\n",
        "            fontweight=weight,  # Use bold or regular\n",
        "            linespacing=1.5,\n",
        "            color=color\n",
        "        )\n",
        "        y_position -= 0.05  # Adjust spacing\n",
        "    images = []\n",
        "    plot_type='others'\n",
        "    images.append((plot_to_base64(fig), plot_type))\n",
        "    return images\n",
        "\n",
        "    # Show the report image\n",
        "    # plt.show()\n",
        "\n",
        "def is_id_column(df, col_name):\n",
        "    \"\"\"\n",
        "    Detects ID-like columns dynamically based on uniqueness.\n",
        "    If more than 50% of values are unique, the column is likely an identifier.\n",
        "    \"\"\"\n",
        "    return df[col_name].nunique() > (0.5 * len(df))\n",
        "\n",
        "def plot_kde_with_insights(df):\n",
        "    \"\"\"\n",
        "    Generates Kernel Density Estimation (KDE) plots for numerical columns,\n",
        "    filters out ID-like columns, analyzes variance, and annotates insights using a legend.\n",
        "    \"\"\"\n",
        "    numerical_cols = [col for col in df.select_dtypes(include=[np.number]).columns if not is_id_column(df, col)]\n",
        "\n",
        "    if not numerical_cols:\n",
        "        print(\"No suitable numerical columns found for KDE plotting.\")\n",
        "        return\n",
        "    results = []\n",
        "    for col in numerical_cols:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.kdeplot(df[col], shade=True, color=DARK_COLORS[2], linewidth=2, label=f\"Distribution of {col}\")\n",
        "\n",
        "        # Calculate variance and standard deviation\n",
        "        variance = np.var(df[col])\n",
        "        std_dev = np.std(df[col])\n",
        "\n",
        "        # Define variance categories dynamically based on column mean\n",
        "        mean_value = df[col].mean()\n",
        "        low_threshold = mean_value * 0.05\n",
        "        high_threshold = mean_value * 0.5\n",
        "\n",
        "        # Interpret variance levels\n",
        "        if variance > high_threshold:\n",
        "            insight = f\"High Variance: {col} fluctuates significantly.\"\n",
        "            recommendation = f\"Investigate causes (e.g., seasonal trends, outliers).\"\n",
        "            action = f\"Consider segmenting data for clearer patterns.\"\n",
        "        elif variance < low_threshold:\n",
        "            insight = f\"Low Variance: {col} is stable with little change.\"\n",
        "            recommendation = f\"Check for missing diversity in data sources.\"\n",
        "            action = f\"Ensure data reflects realistic variations.\"\n",
        "        else:\n",
        "            insight = f\"Moderate Variance: {col} has some fluctuation.\"\n",
        "            recommendation = f\"Monitor for unusual trends over time.\"\n",
        "            action = f\"Use smoothing techniques if needed.\"\n",
        "\n",
        "        # Plot formatting\n",
        "        plt.title(f\"KDE Plot: {col}\", fontsize=16, fontweight='bold', pad=20)\n",
        "        plt.xlabel(col, fontsize=12)\n",
        "        plt.ylabel(\"Density\", fontsize=12)\n",
        "        plot_type = 'kde'\n",
        "        # Add insights as a legend instead of overlapping text\n",
        "        plt.legend([f\" {insight}\\n {recommendation}\\n {action}\"], loc=\"best\", fontsize=10, frameon=True)\n",
        "        fig = plt.gcf()\n",
        "        results.append((plot_to_base64(fig), plot_type))\n",
        "    return results;\n",
        "        # plt.show()\n",
        "\n",
        "def plot_correlation_matrix(df):\n",
        "    \"\"\"\n",
        "    Plots a correlation matrix with business insights below the graph.\n",
        "    \"\"\"\n",
        "    # Select only numerical columns\n",
        "    numerical_df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "    # Compute correlation matrix\n",
        "    corr_matrix = numerical_df.corr()\n",
        "\n",
        "    # Define color mapping based on correlation strength\n",
        "    cmap = sns.color_palette(DARK_COLORS + LIGHT_COLORS, as_cmap=True)\n",
        "\n",
        "    # Plot the heatmap\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=cmap, linewidths=0.5, vmin=-1, vmax=1)\n",
        "    plt.title(\"Correlation Matrix Analysis\", fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Generate Business Insights\n",
        "    insights = []\n",
        "    for col1 in corr_matrix.columns:\n",
        "        for col2 in corr_matrix.columns:\n",
        "            if col1 != col2:\n",
        "                corr_value = corr_matrix.loc[col1, col2]\n",
        "                if corr_value > 0.7:\n",
        "                    insights.append(f\"{col1} and {col2} have a strong positive correlation ({corr_value:.2f}). This suggests that increasing {col1} will likely increase {col2}.\")\n",
        "                elif corr_value < -0.7:\n",
        "                    insights.append(f\"{col1} and {col2} have a strong negative correlation ({corr_value:.2f}). This means when {col1} increases, {col2} tends to decrease.\")\n",
        "                elif 0.3 < corr_value < 0.7 or -0.7 < corr_value < -0.3:\n",
        "                    insights.append(f\"{col1} and {col2} have a moderate correlation ({corr_value:.2f}). There is a noticeable relationship, but other factors may influence it.\")\n",
        "\n",
        "    # Display Insights Below the Graph\n",
        "    insight_text = \"\\n\".join(insights)\n",
        "    plt.figtext(0.5, -0.3, insight_text, wrap=True, horizontalalignment='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
        "    results = []\n",
        "    fig = plt.gcf()\n",
        "    plot_type = 'correlation'\n",
        "    results.append((plot_to_base64(fig), plot_type));\n",
        "    return results;\n",
        "    # plt.show()\n",
        "\n",
        "def forecast_business_metrics(df):\n",
        "    # Keywords to identify relevant columns\n",
        "    keywords = [\n",
        "        \"sales\", \"profit\", \"revenue\", \"income\", \"return\", \"proceeds\", \"earnings\",\n",
        "        \"yield\", \"incoming\", \"gain\", \"transactions\", \"deals\", \"purchases\",\n",
        "        \"auctions\", \"bargains\", \"trades\", \"buys\", \"negotiations\"\n",
        "    ]\n",
        "\n",
        "    # Identify columns dynamically\n",
        "    target_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in keywords)]\n",
        "\n",
        "    if not target_cols:\n",
        "        print(\"No relevant financial columns found. Forecasting is skipped.\")\n",
        "        return\n",
        "\n",
        "    # Create a Date column from Year, Month, Day\n",
        "    df['Date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
        "\n",
        "    # Aggregate data monthly\n",
        "    df = df.groupby(pd.Grouper(key='Date', freq='M'))[target_cols].sum().reset_index()\n",
        "    images = []\n",
        "    filter_numbers = [6, 9, 12, 18, 24, 36]\n",
        "    for filter_number in filter_numbers:\n",
        "        for col in target_cols:\n",
        "            plt.figure(figsize=(12, 6))  # Separate figure for each metric\n",
        "\n",
        "            # Prepare data for Prophet\n",
        "            data = df[['Date', col]].rename(columns={'Date': 'ds', col: 'y'})\n",
        "\n",
        "            # Fit the Prophet model\n",
        "            model = Prophet()\n",
        "            model.fit(data)\n",
        "\n",
        "            # Create future dates\n",
        "            future = model.make_future_dataframe(periods=filter_number, freq='M')\n",
        "            forecast = model.predict(future)\n",
        "\n",
        "            # Calculate forecast accuracy\n",
        "            actual_values = data['y'].values\n",
        "            predicted_values = model.predict(data)['yhat'].values\n",
        "            mape = mean_absolute_percentage_error(actual_values, predicted_values) * 100\n",
        "            accuracy = 100 - mape  # Accuracy is 100 - MAPE\n",
        "\n",
        "            # Plot results\n",
        "            plt.plot(data['ds'], data['y'], label=f\"Historical {col}\", color=\"#B58ED7\")\n",
        "            plt.plot(forecast['ds'], forecast['yhat'], label=f\"Forecast {col} (Accuracy: {accuracy:.2f}%)\", color=\"#693696\" , linestyle= 'dotted')\n",
        "\n",
        "            plt.title(f\"{col} Forecast\")\n",
        "            plt.xlabel(\"Date\")\n",
        "            plt.ylabel(\"Value\")\n",
        "            plt.legend(loc=\"best\")\n",
        "            fig = plt.gcf()\n",
        "            plot_type='forecast'\n",
        "            images.append((plot_to_base64(fig), plot_type, filter_number))\n",
        "            # plt.show()\n",
        "    return images;\n",
        "\n",
        "@app.exception_handler(Exception)\n",
        "async def global_exception_handler(request, exc):\n",
        "    return JSONResponse(\n",
        "        status_code=500,\n",
        "        content={\n",
        "            \"error\": str(exc),\n",
        "            \"detail\": traceback.format_exc()\n",
        "        }\n",
        "    )\n",
        "class CsvRequest(BaseModel):\n",
        "    cloudinary_url: str\n",
        "\n",
        "\n",
        "@app.post(\"/clean-data\")\n",
        "async def clean_data(csv_request: CsvRequest):\n",
        "    try:\n",
        "        print(f\"Received Cloudinary URL: {csv_request.cloudinary_url}\")\n",
        "        async with httpx.AsyncClient() as client:\n",
        "            response = await client.get(csv_request.cloudinary_url)\n",
        "            if response.status_code != 200:\n",
        "                raise HTTPException(\n",
        "                    status_code=400,\n",
        "                    detail=\"Failed to download CSV from Cloudinary URL\"\n",
        "                )\n",
        "        content = response.content\n",
        "        print(f\"Downloaded content length: {len(content)} bytes\")\n",
        "\n",
        "        # Try different encodings\n",
        "        encodings_to_try = ['latin-1', 'utf-8', 'iso-8859-1', 'cp1252']\n",
        "        data = None\n",
        "\n",
        "        for encoding in encodings_to_try:\n",
        "            try:\n",
        "                print(f\"Trying {encoding} encoding...\")\n",
        "                data = pd.read_csv(io.StringIO(content.decode(encoding)))\n",
        "                print(f\"Successfully read CSV with {encoding} encoding\")\n",
        "                break\n",
        "            except UnicodeDecodeError:\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {encoding}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if data is None:\n",
        "            raise HTTPException(\n",
        "                status_code=400,\n",
        "                detail=\"Could not read CSV file with any supported encoding\"\n",
        "            )\n",
        "\n",
        "        # Step 2: Apply AutoClean\n",
        "        cleaned_data = AutoClean(\n",
        "            data,\n",
        "            mode = 'manual',\n",
        "            missing_num=\"auto\",  # Impute missing numerical data\n",
        "            missing_categ='auto',  # Impute missing categorical data\n",
        "            outliers='auto',  # Detect and handle outliers\n",
        "            duplicates = 'auto',\n",
        "            extract_datetime = 's',\n",
        "        )\n",
        "\n",
        "\n",
        "        df_cleaned = cleaned_data.output\n",
        "        # csv_buffer = io.StringIO()\n",
        "        # df_cleaned.to_csv(csv_buffer, index=False)  # Save DataFrame to CSV format\n",
        "        # csv_buffer.seek(0)  # Reset buffer position\n",
        "\n",
        "        # 5️⃣ Upload the cleaned CSV to Cloudinary\n",
        "        # /// / / upload_result = cloudinary.uploader.upload_large(csv_buffer, resource_type=\"raw\", folder=\"processed_csvs\")\n",
        "        # upload_result = cloudinary.uploader.upload(csv_buffer.getvalue(), resource_type=\"raw\", folder=\"processed_csvs\")\n",
        "\n",
        "        # Convert DataFrame to CSV string and encode it to bytes\n",
        "        csv_str = df_cleaned.to_csv(index=False, sep=',', encoding='utf-8-sig', date_format='%Y-%m-%d')\n",
        "        csv_bytes = csv_str.encode('utf-8-sig')\n",
        "        file_size = len(csv_bytes)  # Get file size in bytes\n",
        "\n",
        "        # Wrap the CSV bytes in a BytesIO stream (so it's not misinterpreted as a file name)\n",
        "        csv_buffer = io.BytesIO(csv_bytes)\n",
        "        csv_buffer.seek(0)\n",
        "\n",
        "        # Define a threshold in bytes (e.g., 10 MB)\n",
        "        THRESHOLD = 10 * 1024 * 1024  # 10 MB\n",
        "\n",
        "        upload_result = None  # Initialize variable to ensure scope\n",
        "\n",
        "        if file_size < THRESHOLD:\n",
        "            # For smaller files, use the standard upload method with the file-like object\n",
        "            upload_result = cloudinary.uploader.upload(\n",
        "                csv_buffer,\n",
        "                resource_type=\"raw\",\n",
        "                folder=\"processed_csvs\"\n",
        "            )\n",
        "        else:\n",
        "            # For larger files, use upload_large with the same binary stream\n",
        "            csv_buffer.seek(0)  # Ensure pointer is at the beginning\n",
        "            upload_result = cloudinary.uploader.upload_large(\n",
        "                csv_buffer,\n",
        "                resource_type=\"raw\",\n",
        "                folder=\"processed_csvs\"\n",
        "            )\n",
        "\n",
        "        return {\n",
        "            \"message\": \"Cleaning completed successfully\",\n",
        "            'cleaned_csv': upload_result[\"secure_url\"]\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {str(e)}\")\n",
        "        print(f\"Traceback: {traceback.format_exc()}\")\n",
        "        raise HTTPException(\n",
        "            status_code=500,\n",
        "            detail={\n",
        "                \"error\": str(e),\n",
        "                \"traceback\": traceback.format_exc()\n",
        "            }\n",
        "    )\n",
        "\n",
        "@app.post(\"/analyze-data\")\n",
        "async def analyze_data(csv_request: CsvRequest):\n",
        "    try:\n",
        "        print(f\"Received Cloudinary URL: {csv_request.cloudinary_url}\")\n",
        "        async with httpx.AsyncClient() as client:\n",
        "            response = await client.get(csv_request.cloudinary_url)\n",
        "            if response.status_code != 200:\n",
        "                raise HTTPException(\n",
        "                    status_code=400,\n",
        "                    detail=\"Failed to download CSV from Cloudinary URL\"\n",
        "                )\n",
        "        content = response.content\n",
        "        print(f\"Downloaded content length: {len(content)} bytes\")\n",
        "\n",
        "        # Try different encodings\n",
        "        encodings_to_try = ['latin-1', 'utf-8', 'iso-8859-1', 'cp1252']\n",
        "        data = None\n",
        "\n",
        "        for encoding in encodings_to_try:\n",
        "            try:\n",
        "                print(f\"Trying {encoding} encoding...\")\n",
        "                data = pd.read_csv(io.StringIO(content.decode(encoding)))\n",
        "                print(f\"Successfully read CSV with {encoding} encoding\")\n",
        "                break\n",
        "            except UnicodeDecodeError:\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {encoding}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if data is None:\n",
        "            raise HTTPException(\n",
        "                status_code=400,\n",
        "                detail=\"Could not read CSV file with any supported encoding\"\n",
        "            )\n",
        "        cleaned_data = AutoClean(\n",
        "            data,\n",
        "            mode = 'manual',\n",
        "            missing_num=\"auto\",  # Impute missing numerical data\n",
        "            missing_categ='auto',  # Impute missing categorical data\n",
        "            outliers='auto',  # Detect and handle outliers\n",
        "            duplicates = 'auto',\n",
        "            extract_datetime = 's')\n",
        "        # Generate visualizations\n",
        "        images = []\n",
        "        # Step 1: Initial exploration\n",
        "        # print(\"Initial Dataset Overview:\\n\")\n",
        "        # print(\"Null values in each column:\\n\", data.isnull().sum())\n",
        "        # print(\"\\nTotal Duplicates:\", data.duplicated().sum())\n",
        "\n",
        "        # data.head()\n",
        "        # print no of rows\n",
        "        # print(\"Number of rows in the dataset:\", len(data))\n",
        "        # print the outliers\n",
        "        # data.describe()\n",
        "        # identify column types\n",
        "        # data.dtypes\n",
        "        df_cleaned = cleaned_data.output\n",
        "################################################################################################################################\n",
        "        # cleaned csv phase (cloudinary)\n",
        "        csv_str = df_cleaned.to_csv(index=False)\n",
        "        csv_bytes = csv_str.encode('utf-8')\n",
        "        file_size = len(csv_bytes)  # Get file size in bytes\n",
        "\n",
        "        # Wrap the CSV bytes in a BytesIO stream (so it's not misinterpreted as a file name)\n",
        "        csv_buffer = io.BytesIO(csv_bytes)\n",
        "        csv_buffer.seek(0)\n",
        "\n",
        "        # Define a threshold in bytes (e.g., 10 MB)\n",
        "        THRESHOLD = 10 * 1024 * 1024  # 10 MB\n",
        "\n",
        "        upload_result = None  # Initialize variable to ensure scope\n",
        "\n",
        "        if file_size < THRESHOLD:\n",
        "            # For smaller files, use the standard upload method with the file-like object\n",
        "            upload_result = cloudinary.uploader.upload(\n",
        "                csv_buffer,\n",
        "                resource_type=\"raw\",\n",
        "                folder=\"processed_csvs\"\n",
        "            )\n",
        "        else:\n",
        "            # For larger files, use upload_large with the same binary stream\n",
        "            csv_buffer.seek(0)  # Ensure pointer is at the beginning\n",
        "            upload_result = cloudinary.uploader.upload_large(\n",
        "                csv_buffer,\n",
        "                resource_type=\"raw\",\n",
        "                folder=\"processed_csvs\"\n",
        "            )\n",
        "################################################################################################################################################\n",
        "        # data generation\n",
        "        # Define color palettes\n",
        "        DARK_COLORS = [\"#1A0D26\", \"#351B4B\", \"#4F2871\", \"#693696\"]\n",
        "        LIGHT_COLORS = [\"#9C69C9\", \"#B58ED7\", \"#CEB4E4\", \"#E6D9F2\"]\n",
        "        images = []\n",
        "        images.extend(plot_top_categorical_counts_with_dynamic_insights(df_cleaned))\n",
        "        images.extend(plot_top_numerical_insights(df_cleaned))\n",
        "        images.extend(generate_summary_report_image(df_cleaned))\n",
        "        images.extend(plot_kde_with_insights(df_cleaned))\n",
        "        images.extend(plot_correlation_matrix(df_cleaned))\n",
        "        forecast_images = forecast_business_metrics(df_cleaned)\n",
        "        if forecast_images:\n",
        "            images.extend(forecast_images)\n",
        "        return {\n",
        "            \"message\": \"Analysis completed successfully\",\n",
        "            \"images\": images,\n",
        "            \"cleaned_csv\": upload_result[\"secure_url\"]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {str(e)}\")\n",
        "        print(f\"Traceback: {traceback.format_exc()}\")\n",
        "        raise HTTPException(\n",
        "            status_code=500,\n",
        "            detail={\n",
        "                \"error\": str(e),\n",
        "                \"traceback\": traceback.format_exc()\n",
        "            }\n",
        "    )\n",
        "\n",
        "# Setup ngrok\n",
        "ngrok.set_auth_token(\"2swgwcEJ5hsXEst7a5WBLtv58s8_5FtZDTirtSBKrSL4e8HUR\")  # Replace with your ngrok auth token\n",
        "\n",
        "# Run the FastAPI app\n",
        "if __name__ == \"__main__\":\n",
        "  public_url = ngrok.connect(8000, bind_tls=True).public_url\n",
        "  print(f\"FastAPI is publicly accessible at: {public_url}\")\n",
        "  uvicorn.run(\n",
        "      \"__main__:app\",\n",
        "      host=\"0.0.0.0\",\n",
        "      port=8000,\n",
        "      log_config=None,  # Disable Uvicorn's default logging\n",
        "      access_log=False  # Disable access logs\n",
        "  )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}