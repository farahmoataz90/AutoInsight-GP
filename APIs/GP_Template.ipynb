{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPttXdjrXwNH",
        "outputId": "39abe445-6d39-4d6f-ebc1-53f3e73e5575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI is publicly accessible at: https://4b82-35-227-122-72.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-04-29T00:43:07+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8000-1f11095f-7a71-46c5-a333-27ba08e45dfb acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-04-29T00:43:07+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8000-1f11095f-7a71-46c5-a333-27ba08e45dfb err=\"failed to start tunnel: session closed\"\n"
          ]
        }
      ],
      "source": [
        "DARK_COLORS = [\"#1A0D26\", \"#351B4B\", \"#4F2871\", \"#693696\"]\n",
        "LIGHT_COLORS = [\"#9C69C9\", \"#B58ED7\", \"#CEB4E4\", \"#E6D9F2\"]\n",
        "OUTLIER_COLOR = \"#FF5733\"\n",
        "\n",
        "# Apply nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "cloudinary.config(\n",
        "    cloud_name=\"dwd6kau8a\",\n",
        "    api_key=\"414118375842875\",\n",
        "    api_secret=\"99IAqTayxvBkd2aC5DVY1kj1jR0\"\n",
        ")\n",
        "\n",
        "# CORS middleware configuration\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "def plot_to_base64(fig):\n",
        "    \"\"\"Convert matplotlib plot to base64 string\"\"\"\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format='png', bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "    plt.close(fig)\n",
        "    return image_base64\n",
        "\n",
        "def is_likely_id_column(df, column):\n",
        "    \"\"\"Identifies if a column is likely an ID based on unique values ratio.\"\"\"\n",
        "    unique_ratio = df[column].nunique() / len(df)\n",
        "    return unique_ratio > 0.5  # ID-like columns have high uniqueness\n",
        "\n",
        "def generate_dynamic_recommendations(df, column):\n",
        "    \"\"\"Generates data-driven recommendations for a categorical column.\"\"\"\n",
        "    top_values = df[column].value_counts().head(3)\n",
        "    total_count = len(df)\n",
        "\n",
        "    insights = []\n",
        "    for i, (value, count) in enumerate(top_values.items(), 1):\n",
        "        percentage = count / total_count * 100\n",
        "        insights.append(f\"#{i}: '{value}' ({percentage:.1f}%)\")\n",
        "\n",
        "    # Dynamic recommendations based on patterns\n",
        "    if top_values.iloc[0] / total_count > 0.5:\n",
        "        recommendations = \"One value dominates—consider diversifying or investigating bias.\"\n",
        "    elif len(df[column].unique()) > 20:\n",
        "        recommendations = \"Many unique values—group similar categories for better insights.\"\n",
        "    else:\n",
        "        recommendations = \"Balanced distribution—use for segmentation & targeting.\"\n",
        "\n",
        "    return \"\\n\".join(insights) + \"\\n\\n\" + recommendations\n",
        "\n",
        "def generate_dynamic_categorical_insights(df, column):\n",
        "    \"\"\"Generates dynamic insights based on categorical distribution, including top 3 values.\"\"\"\n",
        "    category_counts = df[column].value_counts(normalize=True)\n",
        "    total_values = len(df)\n",
        "    unique_values = df[column].nunique()\n",
        "\n",
        "    top_category = category_counts.idxmax()\n",
        "    top_category_percentage = category_counts.max()\n",
        "\n",
        "    insights = []\n",
        "\n",
        "    if top_category_percentage > 0.5:\n",
        "        insights.append(f\"{column} is dominated by '{top_category}' ({top_category_percentage:.1%}).\\n\"\n",
        "                        f\"Consider diversifying strategies to balance market share.\")\n",
        "\n",
        "    if unique_values > 50 and top_category_percentage < 0.05:\n",
        "        insights.append(f\"{column} has {unique_values} unique categories, none dominant.\\n\"\n",
        "                        f\"Clustering (e.g., K-Means) may reveal hidden patterns.\")\n",
        "\n",
        "    if unique_values > 5 and top_category_percentage < 0.3:\n",
        "        insights.append(f\"{column} is well-distributed with {unique_values} unique values.\\n\"\n",
        "                        f\"Explore correlations with key business metrics like revenue.\")\n",
        "\n",
        "    if (category_counts < 0.01).sum() > unique_values * 0.5:\n",
        "        insights.append(f\"{column} has many low-frequency categories.\\n\"\n",
        "                        f\"Identify if these are niche products, seasonal trends, or data errors.\")\n",
        "\n",
        "    if unique_values < total_values * 0.05:\n",
        "        insights.append(f\"{column} likely represents key attributes like product types.\\n\"\n",
        "                        f\"Use this for targeted marketing and inventory optimization.\")\n",
        "\n",
        "    # Add the top 3 values and their percentages\n",
        "    top_3_values = category_counts.head(3)\n",
        "    top_3_text = \"\\n\".join([f\"{i+1}. '{val}' - {perc:.1%}\" for i, (val, perc) in enumerate(top_3_values.items())])\n",
        "\n",
        "    return \"\\n\".join(insights) + \"\\n\\nTop 3 Values:\\n\" + top_3_text if insights else f\"'{column}' contains meaningful insights.\\n\\nTop 3 Values:\\n{top_3_text}\"\n",
        "\n",
        "def generate_numerical_insights_ecommerce(df, column):\n",
        "    \"\"\"Generates insights based on numerical distributions, skewness, and outliers.\"\"\"\n",
        "    insights = []\n",
        "    mean_val = df[column].mean()\n",
        "    median_val = df[column].median()\n",
        "    std_dev = df[column].std()\n",
        "    skewness = df[column].skew()\n",
        "    kurtosis = df[column].kurtosis()\n",
        "\n",
        "    insights.append(f\"Mean: {mean_val:.2f}, Median: {median_val:.2f}, Std Dev: {std_dev:.2f}\")\n",
        "\n",
        "    if abs(skewness) > 1:\n",
        "        insights.append(f\"Highly skewed (Skewness: {skewness:.2f})\")\n",
        "    elif abs(skewness) > 0.5:\n",
        "        insights.append(f\"Moderately skewed (Skewness: {skewness:.2f})\")\n",
        "    else:\n",
        "        insights.append(f\"Approximately Normal distribution (Skewness: {skewness:.2f})\")\n",
        "\n",
        "    if kurtosis > 3:\n",
        "        insights.append(f\"High peak and heavy tails (Kurtosis: {kurtosis:.2f})\")\n",
        "    elif kurtosis < -1:\n",
        "        insights.append(f\"Flatter distribution (Kurtosis: {kurtosis:.2f})\")\n",
        "\n",
        "    q1, q3 = df[column].quantile([0.25, 0.75])\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column]\n",
        "\n",
        "    if not outliers.empty:\n",
        "        insights.append(f\"Outliers detected ({len(outliers)} values outside IQR range)\")\n",
        "\n",
        "    return \"\\n\".join(insights)\n",
        "\n",
        "def plot_top_categorical_counts_with_dynamic_insights_ecommerce(df):\n",
        "    \"\"\"\n",
        "    Plots the top 10 categories based on count.\n",
        "    Uses:\n",
        "    - Pie charts for categorical columns with exactly 3 unique classes.\n",
        "    - Bar charts for other categorical columns.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    filter_numbers=[5, 10, 15, 20]\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "    categorical_columns = [col for col in categorical_columns if not is_likely_id_column(df, col)]\n",
        "\n",
        "    for column in categorical_columns:\n",
        "            category_counts = df[column].value_counts().nlargest(10)\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            if len(df[column].unique()) <= 3:  # Use pie chart if exactly 3 unique categories\n",
        "                plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', colors=LIGHT_COLORS)\n",
        "                plt.title(f\"Distribution of {column} (Count)\")\n",
        "                plot_type = \"pie_chart\"\n",
        "                insight_text = generate_dynamic_recommendations(df, column)\n",
        "                plt.figtext(0.5, -0.25, insight_text, wrap=True, horizontalalignment='center', fontsize=10,\n",
        "                    bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3'))\n",
        "                plt.tight_layout()\n",
        "                fig = plt.gcf()\n",
        "                results.append((plot_to_base64(fig), plot_type))\n",
        "    for filter_number in filter_numbers:\n",
        "        for column in categorical_columns:\n",
        "            category_counts = df[column].value_counts().nlargest(filter_number)\n",
        "\n",
        "            plt.figure(figsize=(10, 5))\n",
        "\n",
        "            if len(df[column].unique()) != 3:  # Use bar chart for all other cases\n",
        "                sns.barplot(y=category_counts.index, x=category_counts.values, palette=DARK_COLORS + LIGHT_COLORS)\n",
        "                plt.xlabel(\"Count\")\n",
        "                plt.ylabel(column)\n",
        "                plt.title(f\"Top {filter_number} {column} Categories (by Count)\")\n",
        "                plot_type = \"bar_chart\"\n",
        "                # Generate insights and recommendations\n",
        "                insight_text = generate_dynamic_categorical_insights(df, column)\n",
        "                plt.figtext(0.5, -0.25, insight_text, wrap=True, horizontalalignment='center', fontsize=10,\n",
        "                    bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3'))\n",
        "                plt.tight_layout()\n",
        "                fig = plt.gcf()\n",
        "                results.append((plot_to_base64(fig), plot_type, filter_number))\n",
        "\n",
        "            # plt.show()\n",
        "    return results;\n",
        "\n",
        "def plot_top_categorical_counts_with_dynamic_insights_education(df):\n",
        "    \"\"\"\n",
        "    Plots the top 10 categories based on count.\n",
        "    Uses:\n",
        "    - Pie charts for categorical columns with exactly 3 unique classes.\n",
        "    - Bar charts for other categorical columns.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    filter_numbers=[5, 10, 15, 20]\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "    categorical_columns = [col for col in categorical_columns if not is_likely_id_column(df, col)]\n",
        "\n",
        "    for column in categorical_columns:\n",
        "            category_counts = df[column].value_counts().nlargest(10)\n",
        "            unique_values = df[column].nunique()\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            if (unique_values) <= 3:  # Use pie chart if exactly 3 unique categories\n",
        "                plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', colors=LIGHT_COLORS)\n",
        "                plt.title(f\"Distribution of {column} (Count)\")\n",
        "                plot_type = \"pie_chart\"\n",
        "                insight_text = generate_dynamic_categorical_insights(df, column)\n",
        "                plt.figtext(0.5, -0.25, insight_text, wrap=True, horizontalalignment='center', fontsize=10,\n",
        "                    bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3'))\n",
        "                plt.tight_layout()\n",
        "                fig = plt.gcf()\n",
        "                results.append((plot_to_base64(fig), plot_type))\n",
        "    for filter_number in filter_numbers:\n",
        "        for column in categorical_columns:\n",
        "            category_counts = df[column].value_counts().nlargest(filter_number)\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "\n",
        "            if len(df[column].unique()) != 3:  # Use bar chart for all other cases\n",
        "                sns.barplot(y=category_counts.index, x=category_counts.values, palette=DARK_COLORS + LIGHT_COLORS)\n",
        "                plt.xlabel(\"Count\")\n",
        "                plt.ylabel(column)\n",
        "                plt.title(f\"Top {filter_number} {column} Categories (by Count)\")\n",
        "                plot_type = \"bar_chart\"\n",
        "                # Generate insights and recommendations\n",
        "                insight_text = generate_dynamic_categorical_insights(df, column)\n",
        "                plt.figtext(0.5, -0.25, insight_text, wrap=True, horizontalalignment='center', fontsize=10,\n",
        "                    bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3'))\n",
        "                plt.tight_layout()\n",
        "                fig = plt.gcf()\n",
        "                results.append((plot_to_base64(fig), plot_type, filter_number))\n",
        "\n",
        "            # plt.show()\n",
        "    return results;\n",
        "\n",
        "def plot_top_numerical_insights_ecommerce(df):\n",
        "    \"\"\"\n",
        "    Analyzes numerical data and visualizes only meaningful distributions.\n",
        "    - Filters out ID-like columns.\n",
        "    - Uses histograms, line plots, and bar charts dynamically.\n",
        "    \"\"\"\n",
        "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    filter_numbers=[5, 10, 15, 20]\n",
        "    # Remove ID-like columns (those with mostly unique values)\n",
        "    filtered_numerical_columns = [col for col in numerical_columns if df[col].nunique() / len(df) < 0.9]\n",
        "    results = []\n",
        "    for filter_number in filter_numbers:\n",
        "        for column in filtered_numerical_columns:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "\n",
        "            # # Use different types of charts based on the data\n",
        "            # if df[column].nunique() > 50:  # Continuous data -> line chart\n",
        "            #     sns.lineplot(data=df[column], color=DARK_COLORS[0])\n",
        "            #     plt.title(f\"Trend of {column} Over Time\")\n",
        "            #     plt.xlabel(\"Index\")\n",
        "            #     plt.ylabel(column)\n",
        "\n",
        "            # if df[column].nunique() > 5 and df[column].nunique() < 50:  # Moderate categories -> bar chart\n",
        "            #     sns.barplot(x=df[column].value_counts().index[:filter_number], y=df[column].value_counts().values[:filter_number], palette=DARK_COLORS + LIGHT_COLORS)\n",
        "            #     plt.title(f\"Top 10 Values of {column}\")\n",
        "            #     plt.xlabel(column)\n",
        "            #     plt.ylabel(\"Count\")\n",
        "            #     plot_type = \"bar_chart\"\n",
        "            # else:  # Discrete numeric values -> histogram\n",
        "            sns.histplot(df[column], kde=True, color=DARK_COLORS[0], bins=filter_number, label=generate_numerical_insights(df, column))\n",
        "            plt.xlabel(column)\n",
        "            plt.ylabel(\"Density\")\n",
        "            plt.title(f\"Distribution of {column}\")\n",
        "            plot_type = \"histogram\"\n",
        "            plt.legend(loc='upper right', fontsize=10, frameon=True, edgecolor='black')\n",
        "            plt.tight_layout()\n",
        "            fig = plt.gcf()\n",
        "            results.append((plot_to_base64(fig), plot_type, filter_number))\n",
        "    return results;\n",
        "        # plt.show()\n",
        "\n",
        "def generate_summary_report_image_education(df):\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "    insights = []\n",
        "\n",
        "    # Title Section\n",
        "    insights.append((\"STUDENT PERFORMANCE REPORT\\n\", DARK_COLORS[0], \"bold\"))\n",
        "    insights.append((\"This report highlights key trends in student performance to help improve learning outcomes.\\n\\n\\n\\n\", DARK_COLORS[1], \"regular\"))\n",
        "\n",
        "    # Generate insights for each categorical column related to student performance\n",
        "    for column in categorical_columns:\n",
        "        value_counts = df[column].value_counts()\n",
        "        top_value = value_counts.idxmax()\n",
        "        count = value_counts.max()\n",
        "        total = len(df)\n",
        "        percentage = (count / total) * 100\n",
        "\n",
        "        # Define color coding based on percentage\n",
        "        if percentage > 50:\n",
        "            color = LIGHT_COLORS[0]  # Most common category\n",
        "            action = \"This category represents a dominant student group. Ensure engagement continues.\\n\"\n",
        "        elif percentage > 20:\n",
        "            color = LIGHT_COLORS[1]  # Moderate category\n",
        "            action = \"This group is stable. Look for opportunities to boost performance.\\n\"\n",
        "        else:\n",
        "            color = LIGHT_COLORS[2]  # Low-performing category\n",
        "            action = \"This category has lower engagement. Develop strategies to support this group.\\n\"\n",
        "\n",
        "        # Generate formatted insight\n",
        "        insights.append((f\"{column} Insights\", DARK_COLORS[2], \"bold\"))\n",
        "        insights.append((f\"- Most Common: {top_value} ({count} times, {percentage:.1f}% of total)\", DARK_COLORS[3], \"regular\"))\n",
        "        insights.append((f\"- {action}\\n\", color, \"regular\"))\n",
        "\n",
        "    # Key Recommendations for improving student performance\n",
        "    insights.append((\"KEY RECOMMENDATIONS\\n\", DARK_COLORS[0], \"bold\"))\n",
        "    insights.append((\"- Focus on underperforming groups to enhance support and resources.\", DARK_COLORS[1], \"regular\"))\n",
        "    insights.append((\"- Encourage high-performing students to mentor peers and share study techniques.\", DARK_COLORS[1], \"regular\"))\n",
        "    insights.append((\"- Continuously monitor attendance and participation to prevent disengagement.\\n\", DARK_COLORS[1], \"regular\"))\n",
        "\n",
        "    # Summary Section\n",
        "    insights.append((\"SUMMARY\\n\", DARK_COLORS[0], \"bold\"))\n",
        "    insights.append((\"This report provides insights into student engagement and performance. Use this data to refine teaching strategies and offer tailored support. Empower students through data-driven decisions to foster growth and academic success.\", DARK_COLORS[1], \"regular\"))\n",
        "\n",
        "    # Create figure with a white background\n",
        "    fig, ax = plt.subplots(figsize=(12, 8), dpi=100, facecolor=\"white\")\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_frame_on(False)\n",
        "\n",
        "    # Display text inside the figure with different colors and font weights\n",
        "    y_position = 0.95\n",
        "    for text, color, weight in insights:\n",
        "        wrapped_text = \"\\n\".join(wrap(text, width=90))\n",
        "        ax.text(\n",
        "            0.02, y_position, wrapped_text,\n",
        "            fontsize=14,\n",
        "            va=\"top\", ha=\"left\",\n",
        "            family=\"Times New Roman\",\n",
        "            fontweight=weight,  # Use bold or regular\n",
        "            linespacing=1.5,\n",
        "            color=color\n",
        "        )\n",
        "        y_position -= 0.05  # Adjust spacing\n",
        "    images = []\n",
        "    plot_type='others'\n",
        "    images.append((plot_to_base64(fig), plot_type))\n",
        "    return images\n",
        "\n",
        "def generate_summary_report_image_ecommerce(df):\n",
        "    \"\"\"\n",
        "    Generates and displays a structured business insights report with enhanced readability,\n",
        "    color-coded insights, and numerical breakdowns.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): A Pandas DataFrame containing categorical data.\n",
        "    \"\"\"\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "    insights = []\n",
        "\n",
        "    # Title Section\n",
        "    insights.append((\"STUDENT PERFORMANCE REPORT\\n\", DARK_COLORS[0], \"bold\"))\n",
        "    insights.append((\"This report highlights key trends in student performance to help improve learning outcomes.\\n\\n\\n\\n\", DARK_COLORS[1], \"regular\"))\n",
        "\n",
        "    # Generate insights for each categorical column related to student performance\n",
        "    for column in categorical_columns:\n",
        "        value_counts = df[column].value_counts()\n",
        "        top_value = value_counts.idxmax()\n",
        "        count = value_counts.max()\n",
        "        total = len(df)\n",
        "        percentage = (count / total) * 100\n",
        "\n",
        "        # Define color coding based on percentage\n",
        "        if percentage > 50:\n",
        "            color = LIGHT_COLORS[0]  # Most common category\n",
        "            action = \"This category represents a dominant student group. Ensure engagement continues.\\n\"\n",
        "        elif percentage > 20:\n",
        "            color = LIGHT_COLORS[1]  # Moderate category\n",
        "            action = \"This group is stable. Look for opportunities to boost performance.\\n\"\n",
        "        else:\n",
        "            color = LIGHT_COLORS[2]  # Low-performing category\n",
        "            action = \"This category has lower engagement. Develop strategies to support this group.\\n\"\n",
        "\n",
        "        # Generate formatted insight\n",
        "        insights.append((f\"{column} Insights\", DARK_COLORS[2], \"bold\"))\n",
        "        insights.append((f\"- Most Common: {top_value} ({count} times, {percentage:.1f}% of total)\", DARK_COLORS[3], \"regular\"))\n",
        "        insights.append((f\"- {action}\\n\", color, \"regular\"))\n",
        "\n",
        "    # Key Recommendations for improving student performance\n",
        "    insights.append((\"KEY RECOMMENDATIONS\\n\", DARK_COLORS[0], \"bold\"))\n",
        "    insights.append((\"- Focus on underperforming groups to enhance support and resources.\", DARK_COLORS[1], \"regular\"))\n",
        "    insights.append((\"- Encourage high-performing students to mentor peers and share study techniques.\", DARK_COLORS[1], \"regular\"))\n",
        "    insights.append((\"- Continuously monitor attendance and participation to prevent disengagement.\\n\", DARK_COLORS[1], \"regular\"))\n",
        "\n",
        "    # Summary Section\n",
        "    insights.append((\"SUMMARY\\n\", DARK_COLORS[0], \"bold\"))\n",
        "    insights.append((\"This report provides insights into student engagement and performance. Use this data to refine teaching strategies and offer tailored support. Empower students through data-driven decisions to foster growth and academic success.\", DARK_COLORS[1], \"regular\"))\n",
        "\n",
        "    # Create figure with a white background\n",
        "    fig, ax = plt.subplots(figsize=(12, 8), dpi=100, facecolor=\"white\")\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_frame_on(False)\n",
        "\n",
        "    # Display text inside the figure with different colors and font weights\n",
        "    y_position = 0.95\n",
        "    for text, color, weight in insights:\n",
        "        wrapped_text = \"\\n\".join(wrap(text, width=90))\n",
        "        ax.text(\n",
        "            0.02, y_position, wrapped_text,\n",
        "            fontsize=14,\n",
        "            va=\"top\", ha=\"left\",\n",
        "            family=\"Times New Roman\",\n",
        "            fontweight=weight,  # Use bold or regular\n",
        "            linespacing=1.5,\n",
        "            color=color\n",
        "        )\n",
        "        y_position -= 0.05  # Adjust spacing\n",
        "\n",
        "    images = []\n",
        "    plot_type='others'\n",
        "    images.append((plot_to_base64(fig), plot_type))\n",
        "    return images\n",
        "\n",
        "    # Show the report image\n",
        "    # plt.show()\n",
        "\n",
        "def is_id_column(df, col_name):\n",
        "    \"\"\"\n",
        "    Detects ID-like columns dynamically based on uniqueness.\n",
        "    If more than 50% of values are unique, the column is likely an identifier.\n",
        "    \"\"\"\n",
        "    return df[col_name].nunique() > (0.5 * len(df))\n",
        "\n",
        "def plot_kde_with_insights_ecommerce(df):\n",
        "    \"\"\"\n",
        "    Generates Kernel Density Estimation (KDE) plots for numerical columns,\n",
        "    filters out ID-like columns, analyzes variance, and annotates insights using a legend.\n",
        "    \"\"\"\n",
        "    numerical_cols = [col for col in df.select_dtypes(include=[np.number]).columns if not is_id_column(df, col)]\n",
        "\n",
        "    if not numerical_cols:\n",
        "        print(\"No suitable numerical columns found for KDE plotting.\")\n",
        "        return\n",
        "    results = []\n",
        "    for col in numerical_cols:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.kdeplot(df[col], shade=True, color=DARK_COLORS[2], linewidth=2, label=f\"Distribution of {col}\")\n",
        "\n",
        "        # Calculate variance and standard deviation\n",
        "        variance = np.var(df[col])\n",
        "        std_dev = np.std(df[col])\n",
        "\n",
        "        # Define variance categories dynamically based on column mean\n",
        "        mean_value = df[col].mean()\n",
        "        low_threshold = mean_value * 0.05\n",
        "        high_threshold = mean_value * 0.5\n",
        "\n",
        "        # Interpret variance levels\n",
        "        if variance > high_threshold:\n",
        "            insight = f\"High Variance: {col} fluctuates significantly.\"\n",
        "            recommendation = f\"Investigate causes (e.g., seasonal trends, outliers).\"\n",
        "            action = f\"Consider segmenting data for clearer patterns.\"\n",
        "        elif variance < low_threshold:\n",
        "            insight = f\"Low Variance: {col} is stable with little change.\"\n",
        "            recommendation = f\"Check for missing diversity in data sources.\"\n",
        "            action = f\"Ensure data reflects realistic variations.\"\n",
        "        else:\n",
        "            insight = f\"Moderate Variance: {col} has some fluctuation.\"\n",
        "            recommendation = f\"Monitor for unusual trends over time.\"\n",
        "            action = f\"Use smoothing techniques if needed.\"\n",
        "\n",
        "        # Plot formatting\n",
        "        plt.title(f\"KDE Plot: {col}\", fontsize=16, fontweight='bold', pad=20)\n",
        "        plt.xlabel(col, fontsize=12)\n",
        "        plt.ylabel(\"Density\", fontsize=12)\n",
        "        plot_type = 'kde'\n",
        "        # Add insights as a legend instead of overlapping text\n",
        "        plt.legend([f\" {insight}\\n {recommendation}\\n {action}\"], loc=\"best\", fontsize=10, frameon=True)\n",
        "        fig = plt.gcf()\n",
        "        results.append((plot_to_base64(fig), plot_type))\n",
        "    return results;\n",
        "\n",
        "def plot_kde_with_insights_education(df):\n",
        "    \"\"\"\n",
        "    Generates Kernel Density Estimation (KDE) plots for numerical columns,\n",
        "    filters out ID-like columns, analyzes variance, and annotates insights using a legend.\n",
        "    \"\"\"\n",
        "    numerical_cols = [col for col in df.select_dtypes(include=[np.number]).columns if not is_id_column(df, col)]\n",
        "\n",
        "    if not numerical_cols:\n",
        "        print(\"No suitable numerical columns found for KDE plotting.\")\n",
        "        return\n",
        "    results = []\n",
        "    for col in numerical_cols:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.kdeplot(df[col], shade=True, color=DARK_COLORS[2], linewidth=2, label=f\"Distribution of {col}\")\n",
        "\n",
        "        # Calculate variance and standard deviation\n",
        "        variance = np.var(df[col])\n",
        "        std_dev = np.std(df[col])\n",
        "\n",
        "        # Define variance categories dynamically based on column mean\n",
        "        mean_value = df[col].mean()\n",
        "        low_threshold = mean_value * 0.05\n",
        "        high_threshold = mean_value * 0.5\n",
        "\n",
        "         # Interpret variance levels\n",
        "        if variance > high_threshold:\n",
        "            insight = f\"High Variance: {col} shows significant differences in performance.\"\n",
        "            recommendation = f\"Investigate factors like study habits, class participation, or assignments.\"\n",
        "            action = f\"Provide personalized guidance for students who are struggling.\"\n",
        "        elif variance < low_threshold:\n",
        "            insight = f\"Low Variance: {col} shows stable performance across students.\"\n",
        "            recommendation = f\"Ensure consistent assessment methods.\"\n",
        "            action = f\"Look into whether the assessments are appropriately challenging.\"\n",
        "        else:\n",
        "            insight = f\"Moderate Variance: {col} shows some variation in student performance.\"\n",
        "            recommendation = f\"Monitor performance regularly and adapt teaching methods.\"\n",
        "            action = f\"Consider intervention for low-performing students or enrichment for high performers.\"\n",
        "\n",
        "        # Plot formatting\n",
        "        plt.title(f\"KDE Plot: {col}\", fontsize=16, fontweight='bold', pad=20)\n",
        "        plt.xlabel(col, fontsize=12)\n",
        "        plt.ylabel(\"Density\", fontsize=12)\n",
        "        plot_type = 'kde'\n",
        "        # Add insights as a legend instead of overlapping text\n",
        "        plt.legend([f\" {insight}\\n {recommendation}\\n {action}\"], loc=\"best\", fontsize=10, frameon=True)\n",
        "        fig = plt.gcf()\n",
        "        results.append((plot_to_base64(fig), plot_type))\n",
        "    return results;\n",
        "        # plt.show()\n",
        "\n",
        "def plot_correlation_matrix_ecommerce(df):\n",
        "    \"\"\"\n",
        "    Plots a correlation matrix with business insights below the graph.\n",
        "    \"\"\"\n",
        "    # Select only numerical columns\n",
        "    numerical_df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "    # Compute correlation matrix\n",
        "    corr_matrix = numerical_df.corr()\n",
        "\n",
        "    # Define color mapping based on correlation strength\n",
        "    cmap = sns.color_palette(DARK_COLORS + LIGHT_COLORS, as_cmap=True)\n",
        "\n",
        "    # Plot the heatmap\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=cmap, linewidths=0.5, vmin=-1, vmax=1)\n",
        "    plt.title(\"Correlation Matrix Analysis\", fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Generate Business Insights\n",
        "    insights = []\n",
        "    for col1 in corr_matrix.columns:\n",
        "        for col2 in corr_matrix.columns:\n",
        "            if col1 != col2:\n",
        "                corr_value = corr_matrix.loc[col1, col2]\n",
        "                if corr_value > 0.7:\n",
        "                    insights.append(f\"{col1} and {col2} have a strong positive correlation ({corr_value:.2f}). This suggests that increasing {col1} will likely increase {col2}.\")\n",
        "                elif corr_value < -0.7:\n",
        "                    insights.append(f\"{col1} and {col2} have a strong negative correlation ({corr_value:.2f}). This means when {col1} increases, {col2} tends to decrease.\")\n",
        "                elif 0.3 < corr_value < 0.7 or -0.7 < corr_value < -0.3:\n",
        "                    insights.append(f\"{col1} and {col2} have a moderate correlation ({corr_value:.2f}). There is a noticeable relationship, but other factors may influence it.\")\n",
        "\n",
        "    # Display Insights Below the Graph\n",
        "    insight_text = \"\\n\".join(insights)\n",
        "    plt.figtext(0.5, -0.3, insight_text, wrap=True, horizontalalignment='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
        "    results = []\n",
        "    fig = plt.gcf()\n",
        "    plot_type = 'correlation'\n",
        "    results.append((plot_to_base64(fig), plot_type));\n",
        "    return results;\n",
        "\n",
        "def plot_correlation_matrix_education(df):\n",
        "    \"\"\"\n",
        "    Plots a correlation matrix with business insights below the graph.\n",
        "    \"\"\"\n",
        "    # Select only numerical columns\n",
        "    numerical_df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "    # Compute correlation matrix\n",
        "    corr_matrix = numerical_df.corr()\n",
        "\n",
        "    # Define color mapping based on correlation strength\n",
        "    cmap = sns.color_palette(DARK_COLORS + LIGHT_COLORS, as_cmap=True)\n",
        "\n",
        "    # Plot the heatmap\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=cmap, linewidths=0.5, vmin=-1, vmax=1)\n",
        "    plt.title(\"Correlation Matrix Analysis\", fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Generate Business Insights\n",
        "    insights = []\n",
        "    for col1 in corr_matrix.columns:\n",
        "        for col2 in corr_matrix.columns:\n",
        "            if col1 != col2:\n",
        "                corr_value = corr_matrix.loc[col1, col2]\n",
        "                if corr_value > 0.7:\n",
        "                    insights.append(f\"{col1} and {col2} have a strong positive correlation ({corr_value:.2f}). This suggests that improvements in {col1} are likely to improve {col2}, e.g., better study habits may lead to better grades.\")\n",
        "                elif corr_value < -0.7:\n",
        "                    insights.append(f\"{col1} and {col2} have a strong negative correlation ({corr_value:.2f}). This means that as {col1} increases, {col2} tends to decrease, such as more time spent in social activities could reduce study time.\")\n",
        "                elif 0.3 < corr_value < 0.7 or -0.7 < corr_value < -0.3:\n",
        "                    insights.append(f\"{col1} and {col2} have a moderate correlation ({corr_value:.2f}). While there is a noticeable relationship, factors like individual learning styles or external support might also play a role.\")\n",
        "\n",
        "    # Display Insights Below the Graph\n",
        "    insight_text = \"\\n\".join(insights)\n",
        "    plt.figtext(0.5, -0.3, insight_text, wrap=True, horizontalalignment='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
        "    fig = plt.gcf()\n",
        "    plot_type = 'correlation'\n",
        "    result = []\n",
        "    result.append((plot_to_base64(fig), plot_type));\n",
        "    return result;\n",
        "    # plt.show()\n",
        "\n",
        "def forecast_business_metrics_ecommerce(df):\n",
        "    # Keywords to identify relevant columns\n",
        "    keywords = [\n",
        "        \"sales\", \"profit\", \"revenue\", \"income\", \"return\", \"proceeds\", \"earnings\",\n",
        "        \"yield\", \"incoming\", \"gain\", \"transactions\", \"deals\", \"purchases\",\n",
        "        \"auctions\", \"bargains\", \"trades\", \"buys\", \"negotiations\"\n",
        "    ]\n",
        "\n",
        "    # Identify columns dynamically\n",
        "    target_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in keywords)]\n",
        "\n",
        "    if not target_cols:\n",
        "        print(\"No relevant financial columns found. Forecasting is skipped.\")\n",
        "        return\n",
        "\n",
        "    # Create a Date column from Year, Month, Day\n",
        "    df['Date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
        "\n",
        "    # Aggregate data monthly\n",
        "    df = df.groupby(pd.Grouper(key='Date', freq='M'))[target_cols].sum().reset_index()\n",
        "    images = []\n",
        "    filter_numbers = [6, 9, 12, 18, 24, 36]\n",
        "    for filter_number in filter_numbers:\n",
        "        for col in target_cols:\n",
        "            plt.figure(figsize=(12, 6))  # Separate figure for each metric\n",
        "\n",
        "            # Prepare data for Prophet\n",
        "            data = df[['Date', col]].rename(columns={'Date': 'ds', col: 'y'})\n",
        "\n",
        "            # Fit the Prophet model\n",
        "            model = Prophet()\n",
        "            model.fit(data)\n",
        "\n",
        "            # Create future dates\n",
        "            future = model.make_future_dataframe(periods=filter_number, freq='M')\n",
        "            forecast = model.predict(future)\n",
        "\n",
        "            # Calculate forecast accuracy\n",
        "            actual_values = data['y'].values\n",
        "            predicted_values = model.predict(data)['yhat'].values\n",
        "            mape = mean_absolute_percentage_error(actual_values, predicted_values) * 100\n",
        "            accuracy = 100 - mape  # Accuracy is 100 - MAPE\n",
        "\n",
        "            # Plot results\n",
        "            plt.plot(data['ds'], data['y'], label=f\"Historical {col}\", color=\"#B58ED7\")\n",
        "            plt.plot(forecast['ds'], forecast['yhat'], label=f\"Forecast {col} (Accuracy: {accuracy:.2f}%)\", color=\"#693696\" , linestyle= 'dotted')\n",
        "\n",
        "            plt.title(f\"{col} Forecast\")\n",
        "            plt.xlabel(\"Date\")\n",
        "            plt.ylabel(\"Value\")\n",
        "            plt.legend(loc=\"best\")\n",
        "            fig = plt.gcf()\n",
        "            plot_type='forecast'\n",
        "            images.append((plot_to_base64(fig), plot_type, filter_number))\n",
        "            # plt.show()\n",
        "    return images;\n",
        "\n",
        "def plot_score_distributions_education(df):\n",
        "    \"\"\"Plots distribution of student scores with stats, outliers, and embedded recommendations.\"\"\"\n",
        "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    filter_numbers=[5, 10, 15, 20]\n",
        "    results=[]\n",
        "    for filter_number in filter_numbers:\n",
        "      for idx, column in enumerate(numerical_cols):\n",
        "        data = df[column].dropna()\n",
        "\n",
        "        # Core stats\n",
        "        mean_val = data.mean()\n",
        "        median_val = data.median()\n",
        "        std_dev = data.std()\n",
        "        variance = data.var()\n",
        "        skewness = data.skew()\n",
        "\n",
        "        # Distribution type\n",
        "        if abs(skewness) < 0.5:\n",
        "            dist_type = \"Approximately Normal\"\n",
        "        elif skewness > 0.5:\n",
        "            dist_type = \"Right Skewed\"\n",
        "        else:\n",
        "            dist_type = \"Left Skewed\"\n",
        "        # Outlier detection (IQR)\n",
        "        q1, q3 = data.quantile([0.25, 0.75])\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "\n",
        "        # Stats legend text\n",
        "        legend_text = (\n",
        "            f\"Mean: {mean_val:.2f}\\n\"\n",
        "            f\"Median: {median_val:.2f}\\n\"\n",
        "            f\"Std Dev: {std_dev:.2f}\\n\"\n",
        "            f\"Variance: {variance:.2f}\\n\"\n",
        "            f\"Skewness: {skewness:.2f} ({dist_type})\\n\"\n",
        "            f\"Outliers: {len(outliers)}\"\n",
        "        )\n",
        "\n",
        "        # Recommendation box text\n",
        "        recommendations = []\n",
        "        if variance > 100:\n",
        "            recommendations.append(\"- High variance detected.\")\n",
        "        if abs(skewness) > 1:\n",
        "            recommendations.append(\"- Highly skewed distribution. Consider normalization.\")\n",
        "        elif abs(skewness) > 0.5:\n",
        "            recommendations.append(\"- Moderate skew. Watch for model bias.\")\n",
        "        else:\n",
        "            recommendations.append(\"- Distribution is nearly normal.\")\n",
        "        if not outliers.empty:\n",
        "            recommendations.append(f\"- {len(outliers)} outliers found. Consider review/cleaning.\")\n",
        "        else:\n",
        "            recommendations.append(\"- No significant outliers.\")\n",
        "\n",
        "        recommendation_text = \"\\n\".join(recommendations)\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(data, kde=True, bins=30, color=DARK_COLORS[idx % len(DARK_COLORS)], label=None)\n",
        "        if not outliers.empty:\n",
        "            sns.rugplot(outliers, height=0.1, color=OUTLIER_COLOR, label=\"Outliers\")\n",
        "\n",
        "        plt.title(f\"Distribution of {column}\", fontsize=14)\n",
        "        plt.xlabel(column)\n",
        "        plt.ylabel(\"Count\")\n",
        "\n",
        "        # Stats legend\n",
        "        plt.legend([legend_text], loc='upper right', fontsize=9, frameon=True, edgecolor='black')\n",
        "\n",
        "        # Add recommendation box below plot\n",
        "        plt.text(\n",
        "            0.5, -0.35, recommendation_text,\n",
        "            ha='center', va='top', transform=plt.gca().transAxes,\n",
        "            fontsize=10, bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"#F2F2F2\", edgecolor=\"#999\")\n",
        "        )\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.05, 1, 1])  # Leave space at bottom for text\n",
        "        fig = plt.gcf()\n",
        "        plot_type='histogram'\n",
        "        results.append((plot_to_base64(fig), plot_type, filter_number))\n",
        "    return results\n",
        "\n",
        "def plot_pca_clusters_education(df):\n",
        "    numerical_cols = df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "    if numerical_cols.shape[1] < 2:\n",
        "        print(\"Not enough numerical features for PCA.\")\n",
        "        return\n",
        "\n",
        "    # Standardize\n",
        "    X = StandardScaler().fit_transform(numerical_cols)\n",
        "\n",
        "    # Apply PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(X)\n",
        "\n",
        "    # KMeans for clustering\n",
        "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "    clusters = kmeans.fit_predict(X)\n",
        "\n",
        "    # Custom colors and labels\n",
        "    DARK_COLORS = [\"#1A0D26\", \"#9C69C9\", \"#4F2871\"]\n",
        "    cluster_colors = DARK_COLORS[:3]  # Use first three dark colors for clusters\n",
        "\n",
        "    cluster_labels = {\n",
        "        0: 'Group A - Possibly High Achievers',\n",
        "        1: 'Group B - Average Performers',\n",
        "        2: 'Group C - Students Needing Support'\n",
        "    }\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1],\n",
        "                         c=clusters, cmap=plt.cm.colors.ListedColormap(cluster_colors),\n",
        "                         edgecolor='k', s=80, alpha=0.8)\n",
        "\n",
        "    # Create legend\n",
        "    handles = [plt.Line2D([0], [0], marker='o', color='w',\n",
        "                          markerfacecolor=cluster_colors[i],\n",
        "                          markersize=10,\n",
        "                          label=f'Cluster {i}: {cluster_labels[i]}')\n",
        "               for i in cluster_labels]\n",
        "\n",
        "    plt.legend(handles=handles, title=\"Student Groups\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    # More understandable axis labels\n",
        "    plt.xlabel(f\"Academic Performance Dimension 1 ({pca.explained_variance_ratio_[0]*100:.1f}% of variability)\")\n",
        "    plt.ylabel(f\"Academic Performance Dimension 2 ({pca.explained_variance_ratio_[1]*100:.1f}% of variability)\")\n",
        "\n",
        "    plt.title(\"Student Performance Analysis: Grouping Similar Students\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    fig = plt.gcf()\n",
        "    result=[]\n",
        "    plot_type='others'\n",
        "    result.append((plot_to_base64(fig), plot_type))\n",
        "    return result;\n",
        "\n",
        "def predictive_analysis_student_data(df):\n",
        "\n",
        "\n",
        "    # Custom colors\n",
        "    DARK_COLORS = [\"#1A0D26\", \"#351B4B\", \"#4F2871\", \"#693696\"]\n",
        "    LIGHT_COLORS = [\"#9C69C9\", \"#B58ED7\", \"#CEB4E4\", \"#E6D9F2\"]\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Clean and encode\n",
        "    df.dropna(axis=0, thresh=int(0.7 * df.shape[1]), inplace=True)\n",
        "    df.dropna(axis=1, thresh=int(0.7 * df.shape[0]), inplace=True)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "    num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    label_encoders = {}\n",
        "    for col in cat_cols:\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "    # Automatically pick a target\n",
        "    potential_targets = [col for col in df.columns if df[col].nunique() < 20 or df[col].dtype in [np.float64, np.int64]]\n",
        "    target_col = potential_targets[-1]\n",
        "\n",
        "    X = df.drop(columns=[target_col])\n",
        "    y = df[target_col]\n",
        "    feature_names = X.columns\n",
        "    X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    if len(np.unique(y)) <= 10:\n",
        "        model = RandomForestClassifier(random_state=42)\n",
        "        model_type = 'classification'\n",
        "    else:\n",
        "        model = RandomForestRegressor(random_state=42)\n",
        "        model_type = 'regression'\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # 1. Feature Importance\n",
        "    plt.subplot(1, 3, 1)\n",
        "    importances = model.feature_importances_\n",
        "    sorted_idx = np.argsort(importances)[::-1]\n",
        "    sns.barplot(\n",
        "        x=importances[sorted_idx],\n",
        "        y=feature_names[sorted_idx],\n",
        "        palette=LIGHT_COLORS * (len(feature_names) // len(LIGHT_COLORS) + 1)\n",
        "    )\n",
        "    plt.title(\"What Factors Matter Most?\")\n",
        "    plt.xlabel(\"Impact Level\")\n",
        "    plt.ylabel(\"Inputs (e.g., behavior, scores)\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.4)\n",
        "\n",
        "    # 2. Actual vs Predicted or Confusion Matrix\n",
        "    plt.subplot(1, 3, 2)\n",
        "    if model_type == 'regression':\n",
        "        sns.scatterplot(x=y_test, y=y_pred, color=LIGHT_COLORS[0], alpha=0.6)\n",
        "        plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
        "        plt.xlabel(\"Real Outcome (Known)\")\n",
        "        plt.ylabel(\"Model Prediction\")\n",
        "        plt.title(\"How Close Are We to the Real Results?\")\n",
        "    else:\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Purples')\n",
        "        plt.title(\"Prediction Accuracy (Correct vs Wrong)\")\n",
        "        plt.xlabel(\"Predicted Category\")\n",
        "        plt.ylabel(\"Real Category\")\n",
        "\n",
        "    # 3. Summary + Recommendations\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.axis('off')\n",
        "    if model_type == 'regression':\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        plt.text(0.01, 0.9, \"Model Summary (Prediction of Scores)\", fontsize=13, weight='bold')\n",
        "        # print the accuracy in percentage\n",
        "        plt.text(0.05, 0.8, f\"Score accuracy : {r2* 100:.1f}%\")\n",
        "        # plt.text(0.05, 0.55, \"Explanation: This tells us how well the model predicts student performance.\")\n",
        "        plt.text(0.05, 0.7, \"Recommendation: Focus on top influencing features to improve overall scores.\")\n",
        "    else:\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "        accuracy = report['accuracy']\n",
        "        accuracy_pct = accuracy * 100\n",
        "        plt.text(0.01, 0.9, \"Model Summary (Category Prediction)\", fontsize=13, weight='bold')\n",
        "        plt.text(0.05, 0.75, f\"Correct Predictions: {accuracy_pct:.1f}%\")\n",
        "        plt.text(0.05, 0.6, \"Explanation: This percentage shows how many times the model predicted correctly.\")\n",
        "        plt.text(0.05, 0.45, \"Recommendation:\")\n",
        "        plt.text(0.07, 0.35, \"- Investigate misclassified cases for improvement.\")\n",
        "        plt.text(0.07, 0.25, \"- Use top features to guide student support strategies.\")\n",
        "        plt.text(0.07, 0.15, \"- Consider deeper analysis into the most influential inputs.\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(\"Predictive Insights: Understanding Student Performance\", fontsize=18, y=1.08)\n",
        "    images=[]\n",
        "    plot_type='others'\n",
        "    fig=plt.gcf()\n",
        "    images.append((plot_to_base64(fig), plot_type))\n",
        "    return images;\n",
        "\n",
        "\n",
        "@app.exception_handler(Exception)\n",
        "async def global_exception_handler(request, exc):\n",
        "    return JSONResponse(\n",
        "        status_code=500,\n",
        "        content={\n",
        "            \"error\": str(exc),\n",
        "            \"detail\": traceback.format_exc()\n",
        "        }\n",
        "    )\n",
        "\n",
        "class CsvRequest(BaseModel):\n",
        "    cloudinary_url: str\n",
        "\n",
        "class insightsRequest(BaseModel):\n",
        "    cloudinary_url: str\n",
        "    domainType: str\n",
        "\n",
        "@app.post(\"/clean-data\")\n",
        "async def clean_data(csv_request: CsvRequest):\n",
        "    try:\n",
        "        print(f\"Received Cloudinary URL: {csv_request.cloudinary_url}\")\n",
        "        async with httpx.AsyncClient() as client:\n",
        "            response = await client.get(csv_request.cloudinary_url)\n",
        "            if response.status_code != 200:\n",
        "                raise HTTPException(\n",
        "                    status_code=400,\n",
        "                    detail=\"Failed to download CSV from Cloudinary URL\"\n",
        "                )\n",
        "        content = response.content\n",
        "        print(f\"Downloaded content length: {len(content)} bytes\")\n",
        "\n",
        "        # Try different encodings\n",
        "        encodings_to_try = ['latin-1', 'utf-8', 'iso-8859-1', 'cp1252']\n",
        "        data = None\n",
        "\n",
        "        for encoding in encodings_to_try:\n",
        "            try:\n",
        "                print(f\"Trying {encoding} encoding...\")\n",
        "                data = pd.read_csv(io.StringIO(content.decode(encoding)))\n",
        "                print(f\"Successfully read CSV with {encoding} encoding\")\n",
        "                break\n",
        "            except UnicodeDecodeError:\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {encoding}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if data is None:\n",
        "            raise HTTPException(\n",
        "                status_code=400,\n",
        "                detail=\"Could not read CSV file with any supported encoding\"\n",
        "            )\n",
        "\n",
        "        # Step 2: Apply AutoClean\n",
        "        cleaned_data = AutoClean(\n",
        "            data,\n",
        "            mode = 'manual',\n",
        "            missing_num=\"auto\",  # Impute missing numerical data\n",
        "            missing_categ='auto',  # Impute missing categorical data\n",
        "            outliers='auto',  # Detect and handle outliers\n",
        "            duplicates = 'auto',\n",
        "            extract_datetime = 's',\n",
        "        )\n",
        "\n",
        "\n",
        "        df_cleaned = cleaned_data.output\n",
        "        # csv_buffer = io.StringIO()\n",
        "        # df_cleaned.to_csv(csv_buffer, index=False)  # Save DataFrame to CSV format\n",
        "        # csv_buffer.seek(0)  # Reset buffer position\n",
        "\n",
        "        # 5️⃣ Upload the cleaned CSV to Cloudinary\n",
        "        # /// / / upload_result = cloudinary.uploader.upload_large(csv_buffer, resource_type=\"raw\", folder=\"processed_csvs\")\n",
        "        # upload_result = cloudinary.uploader.upload(csv_buffer.getvalue(), resource_type=\"raw\", folder=\"processed_csvs\")\n",
        "\n",
        "        # Convert DataFrame to CSV string and encode it to bytes\n",
        "        csv_str = df_cleaned.to_csv(index=False, sep=',', encoding='utf-8-sig', date_format='%Y-%m-%d')\n",
        "        csv_bytes = csv_str.encode('utf-8-sig')\n",
        "        file_size = len(csv_bytes)  # Get file size in bytes\n",
        "\n",
        "        # Wrap the CSV bytes in a BytesIO stream (so it's not misinterpreted as a file name)\n",
        "        csv_buffer = io.BytesIO(csv_bytes)\n",
        "        csv_buffer.seek(0)\n",
        "\n",
        "        # Define a threshold in bytes (e.g., 10 MB)\n",
        "        THRESHOLD = 10 * 1024 * 1024  # 10 MB\n",
        "\n",
        "        upload_result = None  # Initialize variable to ensure scope\n",
        "\n",
        "        if file_size < THRESHOLD:\n",
        "            # For smaller files, use the standard upload method with the file-like object\n",
        "            upload_result = cloudinary.uploader.upload(\n",
        "                csv_buffer,\n",
        "                resource_type=\"raw\",\n",
        "                folder=\"processed_csvs\"\n",
        "            )\n",
        "        else:\n",
        "            # For larger files, use upload_large with the same binary stream\n",
        "            csv_buffer.seek(0)  # Ensure pointer is at the beginning\n",
        "            upload_result = cloudinary.uploader.upload_large(\n",
        "                csv_buffer,\n",
        "                resource_type=\"raw\",\n",
        "                folder=\"processed_csvs\"\n",
        "            )\n",
        "\n",
        "        return {\n",
        "            \"message\": \"Cleaning completed successfully\",\n",
        "            'cleaned_csv': upload_result[\"secure_url\"]\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {str(e)}\")\n",
        "        print(f\"Traceback: {traceback.format_exc()}\")\n",
        "        raise HTTPException(\n",
        "            status_code=500,\n",
        "            detail={\n",
        "                \"error\": str(e),\n",
        "                \"traceback\": traceback.format_exc()\n",
        "            }\n",
        "    )\n",
        "\n",
        "@app.post(\"/analyze-data\")\n",
        "async def analyze_data(csv_request: insightsRequest):\n",
        "    try:\n",
        "        print(f\"Received Cloudinary URL: {csv_request.cloudinary_url}\")\n",
        "        async with httpx.AsyncClient() as client:\n",
        "            response = await client.get(csv_request.cloudinary_url)\n",
        "            if response.status_code != 200:\n",
        "                raise HTTPException(\n",
        "                    status_code=400,\n",
        "                    detail=\"Failed to download CSV from Cloudinary URL\"\n",
        "                )\n",
        "        content = response.content\n",
        "        print(f\"Downloaded content length: {len(content)} bytes\")\n",
        "\n",
        "        # Try different encodings\n",
        "        encodings_to_try = ['latin-1', 'utf-8', 'iso-8859-1', 'cp1252']\n",
        "        data = None\n",
        "\n",
        "        for encoding in encodings_to_try:\n",
        "            try:\n",
        "                print(f\"Trying {encoding} encoding...\")\n",
        "                data = pd.read_csv(io.StringIO(content.decode(encoding)))\n",
        "                print(f\"Successfully read CSV with {encoding} encoding\")\n",
        "                break\n",
        "            except UnicodeDecodeError:\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {encoding}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if data is None:\n",
        "            raise HTTPException(\n",
        "                status_code=400,\n",
        "                detail=\"Could not read CSV file with any supported encoding\"\n",
        "            )\n",
        "        cleaned_data = AutoClean(\n",
        "            data,\n",
        "            mode = 'manual',\n",
        "            missing_num=\"auto\",  # Impute missing numerical data\n",
        "            missing_categ='auto',  # Impute missing categorical data\n",
        "            outliers='auto',  # Detect and handle outliers\n",
        "            duplicates = 'auto',\n",
        "            extract_datetime = 's')\n",
        "        # Generate visualizations\n",
        "        images = []\n",
        "        # Step 1: Initial exploration\n",
        "        # print(\"Initial Dataset Overview:\\n\")\n",
        "        # print(\"Null values in each column:\\n\", data.isnull().sum())\n",
        "        # print(\"\\nTotal Duplicates:\", data.duplicated().sum())\n",
        "\n",
        "        # data.head()\n",
        "        # print no of rows\n",
        "        # print(\"Number of rows in the dataset:\", len(data))\n",
        "        # print the outliers\n",
        "        # data.describe()\n",
        "        # identify column types\n",
        "        # data.dtypes\n",
        "        df_cleaned = cleaned_data.output\n",
        "################################################################################################################################\n",
        "        # cleaned csv phase (cloudinary)\n",
        "        csv_str = df_cleaned.to_csv(index=False)\n",
        "        csv_bytes = csv_str.encode('utf-8')\n",
        "        file_size = len(csv_bytes)  # Get file size in bytes\n",
        "\n",
        "        # Wrap the CSV bytes in a BytesIO stream (so it's not misinterpreted as a file name)\n",
        "        csv_buffer = io.BytesIO(csv_bytes)\n",
        "        csv_buffer.seek(0)\n",
        "\n",
        "        # Define a threshold in bytes (e.g., 10 MB)\n",
        "        THRESHOLD = 10 * 1024 * 1024  # 10 MB\n",
        "\n",
        "        upload_result = None  # Initialize variable to ensure scope\n",
        "\n",
        "        if file_size < THRESHOLD:\n",
        "            # For smaller files, use the standard upload method with the file-like object\n",
        "            upload_result = cloudinary.uploader.upload(\n",
        "                csv_buffer,\n",
        "                resource_type=\"raw\",\n",
        "                folder=\"processed_csvs\"\n",
        "            )\n",
        "        else:\n",
        "            # For larger files, use upload_large with the same binary stream\n",
        "            csv_buffer.seek(0)  # Ensure pointer is at the beginning\n",
        "            upload_result = cloudinary.uploader.upload_large(\n",
        "                csv_buffer,\n",
        "                resource_type=\"raw\",\n",
        "                folder=\"processed_csvs\"\n",
        "            )\n",
        "################################################################################################################################################\n",
        "        # data generation\n",
        "        # Define color palettes\n",
        "        DARK_COLORS = [\"#1A0D26\", \"#351B4B\", \"#4F2871\", \"#693696\"]\n",
        "        LIGHT_COLORS = [\"#9C69C9\", \"#B58ED7\", \"#CEB4E4\", \"#E6D9F2\"]\n",
        "        images = []\n",
        "        domainType = csv_request.domainType\n",
        "        if (domainType == 'ecommerce'):\n",
        "          images.extend(plot_top_categorical_counts_with_dynamic_insights_ecommerce(df_cleaned))\n",
        "          images.extend(plot_top_numerical_insights_ecommerce(df_cleaned))\n",
        "          images.extend(generate_summary_report_image_ecommerce(df_cleaned))\n",
        "          kde_images = plot_kde_with_insights_ecommerce(df_cleaned)\n",
        "          if kde_images:\n",
        "            images.extend(kde_images)\n",
        "          images.extend(plot_correlation_matrix_ecommerce(df_cleaned))\n",
        "          forecast_images = forecast_business_metrics_ecommerce(df_cleaned)\n",
        "          if forecast_images:\n",
        "              images.extend(forecast_images)\n",
        "        elif (domainType == 'education'):\n",
        "          images.extend(plot_top_categorical_counts_with_dynamic_insights_education(df_cleaned))\n",
        "          images.extend(plot_score_distributions_education(df_cleaned))\n",
        "          images.extend(generate_summary_report_image_education(df_cleaned))\n",
        "          kde_images = plot_kde_with_insights_education(df_cleaned)\n",
        "          if kde_images:\n",
        "            images.extend(kde_images)\n",
        "          images.extend(plot_correlation_matrix_education(df_cleaned))\n",
        "          images.extend(plot_pca_clusters_education(df_cleaned))\n",
        "          images.extend(predictive_analysis_student_data(df_cleaned))\n",
        "\n",
        "        return {\n",
        "            \"message\": \"Analysis completed successfully\",\n",
        "            \"images\": images,\n",
        "            \"cleaned_csv\": upload_result[\"secure_url\"]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {str(e)}\")\n",
        "        print(f\"Traceback: {traceback.format_exc()}\")\n",
        "        raise HTTPException(\n",
        "            status_code=500,\n",
        "            detail={\n",
        "                \"error\": str(e),\n",
        "                \"traceback\": traceback.format_exc()\n",
        "            }\n",
        "    )\n",
        "\n",
        "class ReviewRequest(BaseModel):\n",
        "    review: str\n",
        "\n",
        "@app.post(\"/predict-review\")\n",
        "def predict_review(request: ReviewRequest):\n",
        "\n",
        "    review = request.review\n",
        "    is_sarcastic = detect_sarcasm(review)\n",
        "    processed = preprocess_text(review)\n",
        "    features = vectorizer.transform([processed])\n",
        "    pred = model.predict(features)[0]\n",
        "\n",
        "    if is_sarcastic and pred != 0:\n",
        "        pred = 0\n",
        "\n",
        "    confidence_scores = model.predict_proba(features)[0]\n",
        "    confidence = round(confidence_scores[pred] * 100, 2)\n",
        "\n",
        "    return {\n",
        "        'sentiment': inv_label_map[pred],\n",
        "        'confidence': confidence,\n",
        "        'sarcasm_detected': is_sarcastic\n",
        "    }\n",
        "\n",
        "# Setup ngrok\n",
        "ngrok.set_auth_token(\"2swgwcEJ5hsXEst7a5WBLtv58s8_5FtZDTirtSBKrSL4e8HUR\")  # Replace with your ngrok auth token\n",
        "\n",
        "# Run the FastAPI app\n",
        "if __name__ == \"__main__\":\n",
        "  public_url = ngrok.connect(8000, bind_tls=True).public_url\n",
        "  print(f\"FastAPI is publicly accessible at: {public_url}\")\n",
        "  uvicorn.run(\n",
        "      \"__main__:app\",\n",
        "      host=\"0.0.0.0\",\n",
        "      port=8000,\n",
        "      log_config=None,  # Disable Uvicorn's default logging\n",
        "      access_log=False  # Disable access logs\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb-L0q0zB0Z7",
        "outputId": "efb5e7af-b497-44db-f3a7-7a8fcfdad345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.5-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Collecting py-AutoClean\n",
            "  Downloading py-AutoClean-1.1.3.tar.gz (9.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting cloudinary\n",
            "  Downloading cloudinary-1.44.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from py-AutoClean) (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from py-AutoClean) (2.0.2)\n",
            "Collecting loguru (from py-AutoClean)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from cloudinary) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.11/dist-packages (from cloudinary) (2.4.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx) (1.3.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->py-AutoClean) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->py-AutoClean) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->py-AutoClean) (3.6.0)\n",
            "Downloading pyngrok-7.2.5-py3-none-any.whl (23 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading cloudinary-1.44.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.3/147.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: py-AutoClean\n",
            "  Building wheel for py-AutoClean (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-AutoClean: filename=py_AutoClean-1.1.3-py3-none-any.whl size=10273 sha256=1315acf6df9423b0e07cb95d6a659e74c389dcb93f232e742f3a2994707d9c22\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/13/2b/023a24f8979c2b917dded511572b55f79d5b3a848a3e6646d1\n",
            "Successfully built py-AutoClean\n",
            "Installing collected packages: uvicorn, python-multipart, pyngrok, loguru, cloudinary, starlette, py-AutoClean, fastapi\n",
            "Successfully installed cloudinary-1.44.0 fastapi-0.115.12 loguru-0.7.3 py-AutoClean-1.1.3 pyngrok-7.2.5 python-multipart-0.0.20 starlette-0.46.2 uvicorn-0.34.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install pyngrok fastapi uvicorn nest_asyncio httpx py-AutoClean pandas matplotlib seaborn python-multipart cloudinary\n",
        "import httpx\n",
        "import csv\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "import pandas as pd\n",
        "from AutoClean import AutoClean\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import io\n",
        "from fastapi.responses import StreamingResponse, JSONResponse\n",
        "import base64\n",
        "import nltk\n",
        "from typing import List, Dict\n",
        "from textwrap import wrap\n",
        "import traceback\n",
        "import numpy as np\n",
        "from pydantic import BaseModel\n",
        "import os\n",
        "import logging\n",
        "import cloudinary\n",
        "import cloudinary.uploader\n",
        "import cloudinary.api\n",
        "import string\n",
        "import re\n",
        "from prophet import Prophet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, mean_absolute_percentage_error, confusion_matrix, mean_squared_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('./Reviews.xlsx')\n",
        "\n",
        "\n",
        "# Data cleaning\n",
        "df.dropna(subset=['review', 'sentiment'], inplace=True)\n",
        "df.drop_duplicates(subset=['review'], inplace=True)\n",
        "# Balance dataset\n",
        "min_count = df['sentiment'].value_counts().min()\n",
        "df = df.groupby('sentiment').sample(n=min_count, random_state=42)\n",
        "df = shuffle(df, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Enhanced sarcasm detection function\n",
        "def detect_sarcasm(text):\n",
        "    sarcasm_keywords = [\n",
        "        # Common phrases\n",
        "        \"just what i needed\", \"truly revolutionary\", \"absolutely nothing\",\n",
        "        \"how wonderful\", \"i love how it\", \"what a surprise\", \"exactly what i hoped for\",\n",
        "        \"nailed it\", \"couldn't be worse\", \"brilliant work\", \"flawless fail\", \"amazing crash\",\n",
        "        # Ironic phrases\n",
        "        \"great job crashing\", \"awesome bug\", \"smooth experience (not)\", \"fantastic... not\",\n",
        "        \"because why not\", \"works like a charm\", \"perfectly useless\", \"so helpful\", \"thanks a lot\",\n",
        "        # Contradictions\n",
        "        \"beautiful design but\", \"sleek and fast but\", \"fast but useless\", \"love it when it fails\",\n",
        "        \"excellent bug collection\", \"fails beautifully\", \"didn't expect much and still disappointed\",\n",
        "        # Tone indicators\n",
        "        \"as if\", \"yeah right\", \"sure thing\", \"whatever\", \"big surprise\"\n",
        "    ]\n",
        "\n",
        "    # Check for exaggerated positive words in negative context\n",
        "    positive_words = [\"amazing\", \"awesome\", \"fantastic\", \"perfect\", \"brilliant\", \"wonderful\"]\n",
        "    negative_context = [\"crash\", \"fail\", \"bug\", \"slow\", \"broken\", \"useless\", \"terrible\"]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Check for sarcasm keywords\n",
        "    if any(phrase in text_lower for phrase in sarcasm_keywords):\n",
        "        return True\n",
        "\n",
        "    # Check for exaggerated positive words in negative context\n",
        "    if any(word in text_lower for word in positive_words) and any(word in text_lower for word in negative_context):\n",
        "        return True\n",
        "\n",
        "    # Check for contrastive conjunctions (but, however, although)\n",
        "    if re.search(r'\\b(but|however|although|yet)\\b', text_lower, re.IGNORECASE):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Apply sarcasm detection\n",
        "df['is_sarcastic'] = df['review'].apply(detect_sarcasm)\n",
        "\n",
        "# Text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Lowercase and remove punctuation (keeping emoticons)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s\\'!?]', '', text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Custom stopwords removal (preserving negation words)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    exclusion_words = ['not','no', 'never', 'none', 'nobody', 'nothing', 'neither',\n",
        "                      'nowhere', 'hardly', 'scarcely', 'barely', 'doesnt', 'dont',\n",
        "                      'cant', 'couldnt', 'wont', 'wouldnt', 'shouldnt', 'isnt',\n",
        "                      'wasnt', 'werent', 'hasnt', 'hadnt']\n",
        "\n",
        "    for word in exclusion_words:\n",
        "        stop_words.discard(word)\n",
        "\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization with POS tagging\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    lemmatized_tokens = []\n",
        "    for word, tag in pos_tags:\n",
        "        pos = get_wordnet_pos(tag)\n",
        "        lemmatized_tokens.append(lemmatizer.lemmatize(word, pos))\n",
        "\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'): return wordnet.ADJ\n",
        "    elif tag.startswith('V'): return wordnet.VERB\n",
        "    elif tag.startswith('N'): return wordnet.NOUN\n",
        "    elif tag.startswith('R'): return wordnet.ADV\n",
        "    else: return wordnet.NOUN\n",
        "\n",
        "# Apply preprocessing\n",
        "df['processed_review'] = df['review'].apply(preprocess_text)\n",
        "\n",
        "# Map sentiments to numerical labels\n",
        "label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
        "inv_label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "df['label'] = df['sentiment'].map(label_map)\n",
        "\n",
        "# Feature extraction with TF-IDF (including bigrams)\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
        "X = vectorizer.fit_transform(df['processed_review'])\n",
        "y = df['label']\n",
        "X_train, X_test, y_train, y_test, sarcasm_train, sarcasm_test = train_test_split(X, y, df['is_sarcastic'], test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "raw_preds = model.predict(X_test)\n",
        "\n",
        "# Adjust predictions for sarcastic reviews\n",
        "final_preds = []\n",
        "for pred, is_sarcastic in zip(raw_preds, sarcasm_test.tolist()):\n",
        "    if is_sarcastic and pred != 0:  # If sarcastic and not already negative\n",
        "        final_preds.append(0)  # Force to negative\n",
        "    else:\n",
        "        final_preds.append(pred)\n",
        "# print(\"\\nClassification Report (After Sarcasm Correction):\\n\")\n",
        "print(classification_report(y_test, final_preds, target_names=['negative', 'neutral', 'positive']))\n",
        "# print(\"Accuracy:\", accuracy_score(y_test, final_preds))\n",
        "\n",
        "\n",
        "import joblib\n",
        "\n",
        "joblib.dump(model, 'sentiment_model.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
        "joblib.dump(label_map, 'label_map.pkl')\n",
        "joblib.dump(inv_label_map, 'inv_label_map.pkl')\n",
        "\n",
        "# predict_sentiment()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO3wBoP7GLCj",
        "outputId": "0c3ef87f-e92f-49dc-cefe-ac8dce5622d7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-1' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.76      0.97      0.85        39\n",
            "     neutral       0.90      0.78      0.83        45\n",
            "    positive       0.98      0.88      0.93        51\n",
            "\n",
            "    accuracy                           0.87       135\n",
            "   macro avg       0.88      0.88      0.87       135\n",
            "weighted avg       0.89      0.87      0.87       135\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['inv_label_map.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NaH4OHhs8wRG",
        "outputId": "c83c6a59-f6cb-4d53-ba55-fa5a2f8fce2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FastAPI is publicly accessible at: https://393b-107-167-183-9.ngrok-free.app\n",
            "Received Cloudinary URL: https://res.cloudinary.com/dwd6kau8a/raw/upload/v1741876656/fy8ptfjyv1hb3zliuz1b.csv\n",
            "Downloaded content length: 2305990 bytes\n",
            "Trying latin-1 encoding...\n",
            "Successfully read CSV with latin-1 encoding\n",
            "AutoClean process completed in 3.895565 seconds\n",
            "Logfile saved to: /content/autoclean.log\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/h3lbljgn.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/2r5__boi.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=38431', 'data', 'file=/tmp/tmph3gyfuo2/h3lbljgn.json', 'init=/tmp/tmph3gyfuo2/2r5__boi.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_model7hi91kpc/prophet_model-20250313143920.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:20 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:20 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/p05dc036.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/nyh98arx.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=40279', 'data', 'file=/tmp/tmph3gyfuo2/p05dc036.json', 'init=/tmp/tmph3gyfuo2/nyh98arx.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_modellw4r2rfl/prophet_model-20250313143921.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:21 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:21 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/spbotuic.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/x4yqyo7t.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=84967', 'data', 'file=/tmp/tmph3gyfuo2/spbotuic.json', 'init=/tmp/tmph3gyfuo2/x4yqyo7t.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_modelrcarmqx0/prophet_model-20250313143922.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:22 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:22 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/b4qbdgk2.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/3h70l7st.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=72114', 'data', 'file=/tmp/tmph3gyfuo2/b4qbdgk2.json', 'init=/tmp/tmph3gyfuo2/3h70l7st.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_modelx7kjwnf7/prophet_model-20250313143922.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:22 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:23 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/dxhwxc03.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/zmiwmu01.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=83840', 'data', 'file=/tmp/tmph3gyfuo2/dxhwxc03.json', 'init=/tmp/tmph3gyfuo2/zmiwmu01.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_model4w66r6p6/prophet_model-20250313143923.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:23 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:24 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/nlb524rx.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/6dwkiyux.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=50643', 'data', 'file=/tmp/tmph3gyfuo2/nlb524rx.json', 'init=/tmp/tmph3gyfuo2/6dwkiyux.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_modelbla1uxnx/prophet_model-20250313143924.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:24 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:24 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/fvb6fp7z.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/63jky0nu.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=69261', 'data', 'file=/tmp/tmph3gyfuo2/fvb6fp7z.json', 'init=/tmp/tmph3gyfuo2/63jky0nu.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_model8lws3wtb/prophet_model-20250313143925.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:25 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:25 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/4yk49cms.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/eew8gcc0.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=37338', 'data', 'file=/tmp/tmph3gyfuo2/4yk49cms.json', 'init=/tmp/tmph3gyfuo2/eew8gcc0.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_model6qrgcfy9/prophet_model-20250313143926.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:26 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:26 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/xzbtuc49.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/n3do8om6.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=73959', 'data', 'file=/tmp/tmph3gyfuo2/xzbtuc49.json', 'init=/tmp/tmph3gyfuo2/n3do8om6.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_modeltv5pri5b/prophet_model-20250313143926.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:26 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:27 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/ig93w76y.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/xff_8n2z.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=4720', 'data', 'file=/tmp/tmph3gyfuo2/ig93w76y.json', 'init=/tmp/tmph3gyfuo2/xff_8n2z.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_modelbsah2cau/prophet_model-20250313143927.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:27 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:28 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/pr_7pmch.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/caqtnm4a.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=89071', 'data', 'file=/tmp/tmph3gyfuo2/pr_7pmch.json', 'init=/tmp/tmph3gyfuo2/caqtnm4a.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_modelrkwbdfay/prophet_model-20250313143928.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:28 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:28 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/tta7er1r.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmph3gyfuo2/zw4740jy.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=94586', 'data', 'file=/tmp/tmph3gyfuo2/tta7er1r.json', 'init=/tmp/tmph3gyfuo2/zw4740jy.json', 'output', 'file=/tmp/tmph3gyfuo2/prophet_model_97o5bl0/prophet_model-20250313143929.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
            "14:39:29 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "14:39:30 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DARK_COLORS = [\"#1A0D26\", \"#351B4B\", \"#4F2871\", \"#693696\"]\n",
        "# LIGHT_COLORS = [\"#9C69C9\", \"#B58ED7\", \"#CEB4E4\", \"#E6D9F2\"]\n",
        "\n",
        "# # Apply nest_asyncio\n",
        "# nest_asyncio.apply()\n",
        "\n",
        "# # Initialize FastAPI app\n",
        "# app = FastAPI()\n",
        "\n",
        "# cloudinary.config(\n",
        "#     cloud_name=\"dwd6kau8a\",\n",
        "#     api_key=\"414118375842875\",\n",
        "#     api_secret=\"99IAqTayxvBkd2aC5DVY1kj1jR0\"\n",
        "# )\n",
        "\n",
        "# # CORS middleware configuration\n",
        "# app.add_middleware(\n",
        "#     CORSMiddleware,\n",
        "#     allow_origins=[\"*\"],\n",
        "#     allow_credentials=True,\n",
        "#     allow_methods=[\"*\"],\n",
        "#     allow_headers=[\"*\"],\n",
        "# )\n",
        "\n",
        "# def plot_to_base64(fig):\n",
        "#     \"\"\"Convert matplotlib plot to base64 string\"\"\"\n",
        "#     buf = io.BytesIO()\n",
        "#     fig.savefig(buf, format='png', bbox_inches='tight')\n",
        "#     buf.seek(0)\n",
        "#     image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "#     plt.close(fig)\n",
        "#     return image_base64\n",
        "\n",
        "# def generate_dynamic_insights(df, column):\n",
        "#     \"\"\"\n",
        "#     Generates automated business insights based on categorical distributions.\n",
        "#     \"\"\"\n",
        "#     unique_values = df[column].nunique()\n",
        "#     total_values = len(df[column])\n",
        "#     top_category = df[column].value_counts().idxmax()\n",
        "#     top_category_percentage = df[column].value_counts(normalize=True).iloc[0]\n",
        "\n",
        "#     # Determine insights based on data patterns\n",
        "#     insights = []\n",
        "\n",
        "#     if unique_values > total_values * 0.5:\n",
        "#         insights.append(f\"{column} appears to be a unique identifier. It is not useful for aggregation but can be used for tracking trends like customer retention and fraud detection.\")\n",
        "\n",
        "#     if top_category_percentage > 0.5:\n",
        "#         insights.append(f\"{column} is dominated by one category ({top_category}, {top_category_percentage:.1%}). Consider diversifying products or marketing efforts to create a balanced market.\")\n",
        "\n",
        "#     if unique_values > 50 and top_category_percentage < 0.05:\n",
        "#         insights.append(f\"{column} has a high number of unique categories with no clear dominance. Clustering techniques (e.g., K-Means) can help identify meaningful groups.\")\n",
        "\n",
        "#     if unique_values > 5 and top_category_percentage < 0.3:\n",
        "#         insights.append(f\"{column} has a well-distributed set of categories. Investigate potential correlations between {column} and key business metrics like revenue or sales.\")\n",
        "\n",
        "#     if (df[column].value_counts(normalize=True) < 0.01).sum() > unique_values * 0.5:\n",
        "#         insights.append(f\"{column} contains many low-frequency categories. Identify if these represent niche products, seasonal trends, or potential data errors.\")\n",
        "\n",
        "#     if unique_values < total_values * 0.05:\n",
        "#         insights.append(f\"{column} likely represents key business attributes such as product types or customer segments. Use this to optimize inventory and targeted marketing.\")\n",
        "\n",
        "#     return \"\\n\".join(insights) if insights else f\"{column} contains meaningful business insights. Further analysis can help optimize business strategies.\"\n",
        "\n",
        "# def generate_numerical_insights(df, column):\n",
        "#     \"\"\"Generates insights based on numerical distributions, skewness, and outliers.\"\"\"\n",
        "#     insights = []\n",
        "#     mean_val = df[column].mean()\n",
        "#     median_val = df[column].median()\n",
        "#     std_dev = df[column].std()\n",
        "#     skewness = df[column].skew()\n",
        "#     kurtosis = df[column].kurtosis()\n",
        "\n",
        "#     insights.append(f\"Mean: {mean_val:.2f}, Median: {median_val:.2f}, Std Dev: {std_dev:.2f}\")\n",
        "\n",
        "#     if abs(skewness) > 1:\n",
        "#         insights.append(f\"Highly skewed (Skewness: {skewness:.2f})\")\n",
        "#     elif abs(skewness) > 0.5:\n",
        "#         insights.append(f\"Moderately skewed (Skewness: {skewness:.2f})\")\n",
        "#     else:\n",
        "#         insights.append(f\"Approximately Normal distribution (Skewness: {skewness:.2f})\")\n",
        "\n",
        "#     if kurtosis > 3:\n",
        "#         insights.append(f\"High peak and heavy tails (Kurtosis: {kurtosis:.2f})\")\n",
        "#     elif kurtosis < -1:\n",
        "#         insights.append(f\"Flatter distribution (Kurtosis: {kurtosis:.2f})\")\n",
        "\n",
        "#     q1, q3 = df[column].quantile([0.25, 0.75])\n",
        "#     iqr = q3 - q1\n",
        "#     lower_bound = q1 - 1.5 * iqr\n",
        "#     upper_bound = q3 + 1.5 * iqr\n",
        "#     outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column]\n",
        "\n",
        "#     if not outliers.empty:\n",
        "#         insights.append(f\"Outliers detected ({len(outliers)} values outside IQR range)\")\n",
        "\n",
        "#     return \"\\n\".join(insights)\n",
        "\n",
        "\n",
        "# def plot_top_categorical_counts_with_dynamic_insights(df):\n",
        "#     \"\"\"\n",
        "#     Plots the top 10 categories based on count.\n",
        "#     Uses:\n",
        "#     - Pie charts for categorical columns with exactly 3 unique classes.\n",
        "#     - Bar charts for other categorical columns.\n",
        "#     \"\"\"\n",
        "#     results = []\n",
        "#     filter_numbers=[5, 10, 15, 20]\n",
        "#     categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "#     for column in categorical_columns:\n",
        "#             category_counts = df[column].value_counts().nlargest(10)\n",
        "#             plt.figure(figsize=(10, 5))\n",
        "#             if len(df[column].unique()) == 3:  # Use pie chart if exactly 3 unique categories\n",
        "#                 plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', colors=LIGHT_COLORS)\n",
        "#                 plt.title(f\"Distribution of {column} (Count)\")\n",
        "#                 plot_type = \"pie_chart\"\n",
        "#                 insight_text = generate_dynamic_insights(df, column)\n",
        "#                 plt.figtext(0.5, -0.25, insight_text, wrap=True, horizontalalignment='center', fontsize=10, bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3'))\n",
        "#                 plt.tight_layout()\n",
        "#                 fig = plt.gcf()\n",
        "#                 results.append((plot_to_base64(fig), plot_type))\n",
        "#     for filter_number in filter_numbers:\n",
        "#         for column in categorical_columns:\n",
        "#             category_counts = df[column].value_counts().nlargest(filter_number)\n",
        "\n",
        "#             plt.figure(figsize=(10, 5))\n",
        "\n",
        "#             if len(df[column].unique()) != 3:  # Use bar chart for all other cases\n",
        "#                 sns.barplot(y=category_counts.index, x=category_counts.values, palette=DARK_COLORS + LIGHT_COLORS)\n",
        "#                 plt.xlabel(\"Count\")\n",
        "#                 plt.ylabel(column)\n",
        "#                 plt.title(f\"Top {filter_number} {column} Categories (by Count)\")\n",
        "#                 plot_type = \"bar_chart\"\n",
        "#                 # Generate insights and recommendations\n",
        "#                 insight_text = generate_dynamic_insights(df, column)\n",
        "#                 plt.figtext(0.5, -0.25, insight_text, wrap=True, horizontalalignment='center', fontsize=10, bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3'))\n",
        "#                 plt.tight_layout()\n",
        "#                 fig = plt.gcf()\n",
        "#                 results.append((plot_to_base64(fig), plot_type, filter_number))\n",
        "\n",
        "#             # plt.show()\n",
        "#     return results;\n",
        "\n",
        "# def plot_top_numerical_insights(df):\n",
        "#     \"\"\"\n",
        "#     Analyzes numerical data and visualizes only meaningful distributions.\n",
        "#     - Filters out ID-like columns.\n",
        "#     - Uses histograms, line plots, and bar charts dynamically.\n",
        "#     \"\"\"\n",
        "#     numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "#     filter_numbers=[5, 10, 15, 20]\n",
        "#     # Remove ID-like columns (those with mostly unique values)\n",
        "#     filtered_numerical_columns = [col for col in numerical_columns if df[col].nunique() / len(df) < 0.9]\n",
        "#     results = []\n",
        "#     for filter_number in filter_numbers:\n",
        "#         for column in filtered_numerical_columns:\n",
        "#             plt.figure(figsize=(10, 5))\n",
        "\n",
        "#             # # Use different types of charts based on the data\n",
        "#             # if df[column].nunique() > 50:  # Continuous data -> line chart\n",
        "#             #     sns.lineplot(data=df[column], color=DARK_COLORS[0])\n",
        "#             #     plt.title(f\"Trend of {column} Over Time\")\n",
        "#             #     plt.xlabel(\"Index\")\n",
        "#             #     plt.ylabel(column)\n",
        "\n",
        "#             if df[column].nunique() > 5 and df[column].nunique() < 50:  # Moderate categories -> bar chart\n",
        "#                 sns.barplot(x=df[column].value_counts().index[:filter_number], y=df[column].value_counts().values[:filter_number], palette=DARK_COLORS + LIGHT_COLORS)\n",
        "#                 plt.title(f\"Top 10 Values of {column}\")\n",
        "#                 plt.xlabel(column)\n",
        "#                 plt.ylabel(\"Count\")\n",
        "#                 plot_type = \"bar_chart\"\n",
        "#             else:  # Discrete numeric values -> histogram\n",
        "#                 sns.histplot(df[column], bins=filter_number, kde=True, color=DARK_COLORS[1])\n",
        "#                 plt.title(f\"Distribution of {column}\")\n",
        "#                 plt.xlabel(column)\n",
        "#                 plt.ylabel(\"Frequency\")\n",
        "#                 plot_type = \"histogram\"\n",
        "\n",
        "#             # Insights and recommendations\n",
        "#             plt.figtext(0.5, -0.25, f\"Insight: {column} shows meaningful business trends.\\nRecommendation: Use this to predict future sales, demand, or operational efficiency.\",\n",
        "#                     wrap=True, horizontalalignment='center', fontsize=10, bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3'))\n",
        "\n",
        "#             plt.tight_layout()\n",
        "#             fig = plt.gcf()\n",
        "#             results.append((plot_to_base64(fig), plot_type, filter_number))\n",
        "#     return results;\n",
        "#         # plt.show()\n",
        "# def generate_summary_report_image(df):\n",
        "#     \"\"\"\n",
        "#     Generates and displays a structured business insights report with enhanced readability,\n",
        "#     color-coded insights, and numerical breakdowns.\n",
        "\n",
        "#     Parameters:\n",
        "#         df (DataFrame): A Pandas DataFrame containing categorical data.\n",
        "#     \"\"\"\n",
        "#     categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "#     insights = []\n",
        "\n",
        "#     # Title Section\n",
        "#     insights.append((\"BUSINESS INSIGHTS REPORT\\n\", \"#1A0D26\", \"bold\"))\n",
        "#     insights.append((\"This report provides key trends with actionable insights to guide business decisions.\\n\\n\\n\\n\", \"#333333\", \"regular\"))\n",
        "\n",
        "#     # Generate insights for each categorical column\n",
        "#     for column in categorical_columns:\n",
        "#         value_counts = df[column].value_counts()\n",
        "#         top_value = value_counts.idxmax()  # Most frequent category\n",
        "#         count = value_counts.max()\n",
        "#         total = len(df)\n",
        "#         percentage = (count / total) * 100\n",
        "\n",
        "#         # Define color coding based on percentage\n",
        "#         if percentage > 50:\n",
        "#             color = \"#B58ED7\"  # Green for dominant categories\n",
        "#             action = \"Recommendation: Consider expanding this category as it dominates the market.\"\n",
        "#         elif percentage > 20:\n",
        "#             color = \"#CEB4E4\"  # Orange for balanced categories\n",
        "#             action = \"Recommendation: Maintain and monitor performance trends in this category.\"\n",
        "#         else:\n",
        "#             color = \"#B22222\"  # Red for underperforming categories\n",
        "#             action = \"Recommendation: Analyze why this category has lower engagement and optimize strategies.\"\n",
        "\n",
        "#         # Generate formatted insight\n",
        "#         insights.append((f\"{column} Analysis\", \"#1A0D26\", \"bold\"))  # Bold column name\n",
        "#         insights.append((f\"- Most Frequent Value: {top_value} ({count} occurrences, {percentage:.1f}% of total)\", \"#333333\", \"regular\"))\n",
        "#         insights.append((f\"- {action}\\n\", color, \"regular\"))\n",
        "\n",
        "#     # Key Actionable Steps\n",
        "#     insights.append((\"KEY ACTIONABLE STEPS\\n\", \"#1A0D26\", \"bold\"))\n",
        "#     insights.append((\"- Focus on high-performing categories to optimize revenue.\", \"#333333\", \"regular\"))\n",
        "#     insights.append((\"- Identify and improve underperforming areas based on trends.\", \"#333333\", \"regular\"))\n",
        "#     insights.append((\"- Use data insights to refine marketing, inventory, and logistics strategies.\\n\", \"#333333\", \"regular\"))\n",
        "\n",
        "#     # Create figure\n",
        "#     fig, ax = plt.subplots(figsize=(12, 8), dpi=100)\n",
        "#     ax.set_xticks([])\n",
        "#     ax.set_yticks([])\n",
        "#     ax.set_frame_on(False)\n",
        "\n",
        "#     # Display text inside the figure with different colors and font weights\n",
        "#     y_position = 0.95\n",
        "#     for text, color, weight in insights:\n",
        "#         wrapped_text = \"\\n\".join(wrap(text, width=90))\n",
        "#         ax.text(\n",
        "#             0.02, y_position, wrapped_text,\n",
        "#             fontsize=14,\n",
        "#             va=\"top\", ha=\"left\",\n",
        "#             family=\"Times New Roman\",\n",
        "#             fontweight=weight,  # Use bold or regular\n",
        "#             linespacing=1.5,\n",
        "#             color=color\n",
        "#         )\n",
        "#         y_position -= 0.05  # Adjust spacing\n",
        "#     images = []\n",
        "#     plot_type='others'\n",
        "#     images.append((plot_to_base64(fig), plot_type))\n",
        "#     return images\n",
        "\n",
        "#     # Show the report image\n",
        "#     # plt.show()\n",
        "\n",
        "# def is_id_column(df, col_name):\n",
        "#     \"\"\"\n",
        "#     Detects ID-like columns dynamically based on uniqueness.\n",
        "#     If more than 50% of values are unique, the column is likely an identifier.\n",
        "#     \"\"\"\n",
        "#     return df[col_name].nunique() > (0.5 * len(df))\n",
        "\n",
        "# def plot_kde_with_insights(df):\n",
        "#     \"\"\"\n",
        "#     Generates Kernel Density Estimation (KDE) plots for numerical columns,\n",
        "#     filters out ID-like columns, analyzes variance, and annotates insights using a legend.\n",
        "#     \"\"\"\n",
        "#     numerical_cols = [col for col in df.select_dtypes(include=[np.number]).columns if not is_id_column(df, col)]\n",
        "\n",
        "#     if not numerical_cols:\n",
        "#         print(\"No suitable numerical columns found for KDE plotting.\")\n",
        "#         return\n",
        "#     results = []\n",
        "#     for col in numerical_cols:\n",
        "#         plt.figure(figsize=(10, 6))\n",
        "#         sns.kdeplot(df[col], shade=True, color=DARK_COLORS[2], linewidth=2, label=f\"Distribution of {col}\")\n",
        "\n",
        "#         # Calculate variance and standard deviation\n",
        "#         variance = np.var(df[col])\n",
        "#         std_dev = np.std(df[col])\n",
        "\n",
        "#         # Define variance categories dynamically based on column mean\n",
        "#         mean_value = df[col].mean()\n",
        "#         low_threshold = mean_value * 0.05\n",
        "#         high_threshold = mean_value * 0.5\n",
        "\n",
        "#         # Interpret variance levels\n",
        "#         if variance > high_threshold:\n",
        "#             insight = f\"High Variance: {col} fluctuates significantly.\"\n",
        "#             recommendation = f\"Investigate causes (e.g., seasonal trends, outliers).\"\n",
        "#             action = f\"Consider segmenting data for clearer patterns.\"\n",
        "#         elif variance < low_threshold:\n",
        "#             insight = f\"Low Variance: {col} is stable with little change.\"\n",
        "#             recommendation = f\"Check for missing diversity in data sources.\"\n",
        "#             action = f\"Ensure data reflects realistic variations.\"\n",
        "#         else:\n",
        "#             insight = f\"Moderate Variance: {col} has some fluctuation.\"\n",
        "#             recommendation = f\"Monitor for unusual trends over time.\"\n",
        "#             action = f\"Use smoothing techniques if needed.\"\n",
        "\n",
        "#         # Plot formatting\n",
        "#         plt.title(f\"KDE Plot: {col}\", fontsize=16, fontweight='bold', pad=20)\n",
        "#         plt.xlabel(col, fontsize=12)\n",
        "#         plt.ylabel(\"Density\", fontsize=12)\n",
        "#         plot_type = 'kde'\n",
        "#         # Add insights as a legend instead of overlapping text\n",
        "#         plt.legend([f\" {insight}\\n {recommendation}\\n {action}\"], loc=\"best\", fontsize=10, frameon=True)\n",
        "#         fig = plt.gcf()\n",
        "#         results.append((plot_to_base64(fig), plot_type))\n",
        "#     return results;\n",
        "#         # plt.show()\n",
        "\n",
        "# def plot_correlation_matrix(df):\n",
        "#     \"\"\"\n",
        "#     Plots a correlation matrix with business insights below the graph.\n",
        "#     \"\"\"\n",
        "#     # Select only numerical columns\n",
        "#     numerical_df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "#     # Compute correlation matrix\n",
        "#     corr_matrix = numerical_df.corr()\n",
        "\n",
        "#     # Define color mapping based on correlation strength\n",
        "#     cmap = sns.color_palette(DARK_COLORS + LIGHT_COLORS, as_cmap=True)\n",
        "\n",
        "#     # Plot the heatmap\n",
        "#     plt.figure(figsize=(12, 8))\n",
        "#     sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=cmap, linewidths=0.5, vmin=-1, vmax=1)\n",
        "#     plt.title(\"Correlation Matrix Analysis\", fontsize=14, fontweight='bold')\n",
        "\n",
        "#     # Generate Business Insights\n",
        "#     insights = []\n",
        "#     for col1 in corr_matrix.columns:\n",
        "#         for col2 in corr_matrix.columns:\n",
        "#             if col1 != col2:\n",
        "#                 corr_value = corr_matrix.loc[col1, col2]\n",
        "#                 if corr_value > 0.7:\n",
        "#                     insights.append(f\"{col1} and {col2} have a strong positive correlation ({corr_value:.2f}). This suggests that increasing {col1} will likely increase {col2}.\")\n",
        "#                 elif corr_value < -0.7:\n",
        "#                     insights.append(f\"{col1} and {col2} have a strong negative correlation ({corr_value:.2f}). This means when {col1} increases, {col2} tends to decrease.\")\n",
        "#                 elif 0.3 < corr_value < 0.7 or -0.7 < corr_value < -0.3:\n",
        "#                     insights.append(f\"{col1} and {col2} have a moderate correlation ({corr_value:.2f}). There is a noticeable relationship, but other factors may influence it.\")\n",
        "\n",
        "#     # Display Insights Below the Graph\n",
        "#     insight_text = \"\\n\".join(insights)\n",
        "#     plt.figtext(0.5, -0.3, insight_text, wrap=True, horizontalalignment='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
        "#     results = []\n",
        "#     fig = plt.gcf()\n",
        "#     plot_type = 'correlation'\n",
        "#     results.append((plot_to_base64(fig), plot_type));\n",
        "#     return results;\n",
        "#     # plt.show()\n",
        "\n",
        "# def forecast_business_metrics(df):\n",
        "#     # Keywords to identify relevant columns\n",
        "#     keywords = [\n",
        "#         \"sales\", \"profit\", \"revenue\", \"income\", \"return\", \"proceeds\", \"earnings\",\n",
        "#         \"yield\", \"incoming\", \"gain\", \"transactions\", \"deals\", \"purchases\",\n",
        "#         \"auctions\", \"bargains\", \"trades\", \"buys\", \"negotiations\"\n",
        "#     ]\n",
        "\n",
        "#     # Identify columns dynamically\n",
        "#     target_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in keywords)]\n",
        "\n",
        "#     if not target_cols:\n",
        "#         print(\"No relevant financial columns found. Forecasting is skipped.\")\n",
        "#         return\n",
        "\n",
        "#     # Create a Date column from Year, Month, Day\n",
        "#     df['Date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
        "\n",
        "#     # Aggregate data monthly\n",
        "#     df = df.groupby(pd.Grouper(key='Date', freq='M'))[target_cols].sum().reset_index()\n",
        "#     images = []\n",
        "#     filter_numbers = [6, 9, 12, 18, 24, 36]\n",
        "#     for filter_number in filter_numbers:\n",
        "#         for col in target_cols:\n",
        "#             plt.figure(figsize=(12, 6))  # Separate figure for each metric\n",
        "\n",
        "#             # Prepare data for Prophet\n",
        "#             data = df[['Date', col]].rename(columns={'Date': 'ds', col: 'y'})\n",
        "\n",
        "#             # Fit the Prophet model\n",
        "#             model = Prophet()\n",
        "#             model.fit(data)\n",
        "\n",
        "#             # Create future dates\n",
        "#             future = model.make_future_dataframe(periods=filter_number, freq='M')\n",
        "#             forecast = model.predict(future)\n",
        "\n",
        "#             # Calculate forecast accuracy\n",
        "#             actual_values = data['y'].values\n",
        "#             predicted_values = model.predict(data)['yhat'].values\n",
        "#             mape = mean_absolute_percentage_error(actual_values, predicted_values) * 100\n",
        "#             accuracy = 100 - mape  # Accuracy is 100 - MAPE\n",
        "\n",
        "#             # Plot results\n",
        "#             plt.plot(data['ds'], data['y'], label=f\"Historical {col}\", color=\"#B58ED7\")\n",
        "#             plt.plot(forecast['ds'], forecast['yhat'], label=f\"Forecast {col} (Accuracy: {accuracy:.2f}%)\", color=\"#693696\" , linestyle= 'dotted')\n",
        "\n",
        "#             plt.title(f\"{col} Forecast\")\n",
        "#             plt.xlabel(\"Date\")\n",
        "#             plt.ylabel(\"Value\")\n",
        "#             plt.legend(loc=\"best\")\n",
        "#             fig = plt.gcf()\n",
        "#             plot_type='forecast'\n",
        "#             images.append((plot_to_base64(fig), plot_type, filter_number))\n",
        "#             # plt.show()\n",
        "#     return images;\n",
        "\n",
        "\n",
        "\n",
        "# @app.exception_handler(Exception)\n",
        "# async def global_exception_handler(request, exc):\n",
        "#     return JSONResponse(\n",
        "#         status_code=500,\n",
        "#         content={\n",
        "#             \"error\": str(exc),\n",
        "#             \"detail\": traceback.format_exc()\n",
        "#         }\n",
        "#     )\n",
        "# class CsvRequest(BaseModel):\n",
        "#     cloudinary_url: str\n",
        "\n",
        "\n",
        "# @app.post(\"/clean-data\")\n",
        "# async def clean_data(csv_request: CsvRequest):\n",
        "#     try:\n",
        "#         print(f\"Received Cloudinary URL: {csv_request.cloudinary_url}\")\n",
        "#         async with httpx.AsyncClient() as client:\n",
        "#             response = await client.get(csv_request.cloudinary_url)\n",
        "#             if response.status_code != 200:\n",
        "#                 raise HTTPException(\n",
        "#                     status_code=400,\n",
        "#                     detail=\"Failed to download CSV from Cloudinary URL\"\n",
        "#                 )\n",
        "#         content = response.content\n",
        "#         print(f\"Downloaded content length: {len(content)} bytes\")\n",
        "\n",
        "#         # Try different encodings\n",
        "#         encodings_to_try = ['latin-1', 'utf-8', 'iso-8859-1', 'cp1252']\n",
        "#         data = None\n",
        "\n",
        "#         for encoding in encodings_to_try:\n",
        "#             try:\n",
        "#                 print(f\"Trying {encoding} encoding...\")\n",
        "#                 data = pd.read_csv(io.StringIO(content.decode(encoding)))\n",
        "#                 print(f\"Successfully read CSV with {encoding} encoding\")\n",
        "#                 break\n",
        "#             except UnicodeDecodeError:\n",
        "#                 continue\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error with {encoding}: {str(e)}\")\n",
        "#                 continue\n",
        "\n",
        "#         if data is None:\n",
        "#             raise HTTPException(\n",
        "#                 status_code=400,\n",
        "#                 detail=\"Could not read CSV file with any supported encoding\"\n",
        "#             )\n",
        "\n",
        "#         # Step 2: Apply AutoClean\n",
        "#         cleaned_data = AutoClean(\n",
        "#             data,\n",
        "#             mode = 'manual',\n",
        "#             missing_num=\"auto\",  # Impute missing numerical data\n",
        "#             missing_categ='auto',  # Impute missing categorical data\n",
        "#             outliers='auto',  # Detect and handle outliers\n",
        "#             duplicates = 'auto',\n",
        "#             extract_datetime = 's',\n",
        "#         )\n",
        "\n",
        "\n",
        "#         df_cleaned = cleaned_data.output\n",
        "#         # csv_buffer = io.StringIO()\n",
        "#         # df_cleaned.to_csv(csv_buffer, index=False)  # Save DataFrame to CSV format\n",
        "#         # csv_buffer.seek(0)  # Reset buffer position\n",
        "\n",
        "#         # 5️⃣ Upload the cleaned CSV to Cloudinary\n",
        "#         # /// / / upload_result = cloudinary.uploader.upload_large(csv_buffer, resource_type=\"raw\", folder=\"processed_csvs\")\n",
        "#         # upload_result = cloudinary.uploader.upload(csv_buffer.getvalue(), resource_type=\"raw\", folder=\"processed_csvs\")\n",
        "\n",
        "#         # Convert DataFrame to CSV string and encode it to bytes\n",
        "#         csv_str = df_cleaned.to_csv(index=False, sep=',', encoding='utf-8-sig', date_format='%Y-%m-%d')\n",
        "#         csv_bytes = csv_str.encode('utf-8-sig')\n",
        "#         file_size = len(csv_bytes)  # Get file size in bytes\n",
        "\n",
        "#         # Wrap the CSV bytes in a BytesIO stream (so it's not misinterpreted as a file name)\n",
        "#         csv_buffer = io.BytesIO(csv_bytes)\n",
        "#         csv_buffer.seek(0)\n",
        "\n",
        "#         # Define a threshold in bytes (e.g., 10 MB)\n",
        "#         THRESHOLD = 10 * 1024 * 1024  # 10 MB\n",
        "\n",
        "#         upload_result = None  # Initialize variable to ensure scope\n",
        "\n",
        "#         if file_size < THRESHOLD:\n",
        "#             # For smaller files, use the standard upload method with the file-like object\n",
        "#             upload_result = cloudinary.uploader.upload(\n",
        "#                 csv_buffer,\n",
        "#                 resource_type=\"raw\",\n",
        "#                 folder=\"processed_csvs\"\n",
        "#             )\n",
        "#         else:\n",
        "#             # For larger files, use upload_large with the same binary stream\n",
        "#             csv_buffer.seek(0)  # Ensure pointer is at the beginning\n",
        "#             upload_result = cloudinary.uploader.upload_large(\n",
        "#                 csv_buffer,\n",
        "#                 resource_type=\"raw\",\n",
        "#                 folder=\"processed_csvs\"\n",
        "#             )\n",
        "\n",
        "#         return {\n",
        "#             \"message\": \"Cleaning completed successfully\",\n",
        "#             'cleaned_csv': upload_result[\"secure_url\"]\n",
        "#         }\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error processing file: {str(e)}\")\n",
        "#         print(f\"Traceback: {traceback.format_exc()}\")\n",
        "#         raise HTTPException(\n",
        "#             status_code=500,\n",
        "#             detail={\n",
        "#                 \"error\": str(e),\n",
        "#                 \"traceback\": traceback.format_exc()\n",
        "#             }\n",
        "#     )\n",
        "\n",
        "\n",
        "\n",
        "# @app.post(\"/analyze-data\")\n",
        "# async def analyze_data(csv_request: CsvRequest):\n",
        "#     try:\n",
        "#         print(f\"Received Cloudinary URL: {csv_request.cloudinary_url}\")\n",
        "#         async with httpx.AsyncClient() as client:\n",
        "#             response = await client.get(csv_request.cloudinary_url)\n",
        "#             if response.status_code != 200:\n",
        "#                 raise HTTPException(\n",
        "#                     status_code=400,\n",
        "#                     detail=\"Failed to download CSV from Cloudinary URL\"\n",
        "#                 )\n",
        "#         content = response.content\n",
        "#         print(f\"Downloaded content length: {len(content)} bytes\")\n",
        "\n",
        "#         # Try different encodings\n",
        "#         encodings_to_try = ['latin-1', 'utf-8', 'iso-8859-1', 'cp1252']\n",
        "#         data = None\n",
        "\n",
        "#         for encoding in encodings_to_try:\n",
        "#             try:\n",
        "#                 print(f\"Trying {encoding} encoding...\")\n",
        "#                 data = pd.read_csv(io.StringIO(content.decode(encoding)))\n",
        "#                 print(f\"Successfully read CSV with {encoding} encoding\")\n",
        "#                 break\n",
        "#             except UnicodeDecodeError:\n",
        "#                 continue\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error with {encoding}: {str(e)}\")\n",
        "#                 continue\n",
        "\n",
        "#         if data is None:\n",
        "#             raise HTTPException(\n",
        "#                 status_code=400,\n",
        "#                 detail=\"Could not read CSV file with any supported encoding\"\n",
        "#             )\n",
        "#         cleaned_data = AutoClean(\n",
        "#             data,\n",
        "#             mode = 'manual',\n",
        "#             missing_num=\"auto\",  # Impute missing numerical data\n",
        "#             missing_categ='auto',  # Impute missing categorical data\n",
        "#             outliers='auto',  # Detect and handle outliers\n",
        "#             duplicates = 'auto',\n",
        "#             extract_datetime = 's')\n",
        "#         # Generate visualizations\n",
        "#         images = []\n",
        "#         # Step 1: Initial exploration\n",
        "#         # print(\"Initial Dataset Overview:\\n\")\n",
        "#         # print(\"Null values in each column:\\n\", data.isnull().sum())\n",
        "#         # print(\"\\nTotal Duplicates:\", data.duplicated().sum())\n",
        "\n",
        "#         # data.head()\n",
        "#         # print no of rows\n",
        "#         # print(\"Number of rows in the dataset:\", len(data))\n",
        "#         # print the outliers\n",
        "#         # data.describe()\n",
        "#         # identify column types\n",
        "#         # data.dtypes\n",
        "#         df_cleaned = cleaned_data.output\n",
        "# ################################################################################################################################\n",
        "#         # cleaned csv phase (cloudinary)\n",
        "#         csv_str = df_cleaned.to_csv(index=False)\n",
        "#         csv_bytes = csv_str.encode('utf-8')\n",
        "#         file_size = len(csv_bytes)  # Get file size in bytes\n",
        "\n",
        "#         # Wrap the CSV bytes in a BytesIO stream (so it's not misinterpreted as a file name)\n",
        "#         csv_buffer = io.BytesIO(csv_bytes)\n",
        "#         csv_buffer.seek(0)\n",
        "\n",
        "#         # Define a threshold in bytes (e.g., 10 MB)\n",
        "#         THRESHOLD = 10 * 1024 * 1024  # 10 MB\n",
        "\n",
        "#         upload_result = None  # Initialize variable to ensure scope\n",
        "\n",
        "#         if file_size < THRESHOLD:\n",
        "#             # For smaller files, use the standard upload method with the file-like object\n",
        "#             upload_result = cloudinary.uploader.upload(\n",
        "#                 csv_buffer,\n",
        "#                 resource_type=\"raw\",\n",
        "#                 folder=\"processed_csvs\"\n",
        "#             )\n",
        "#         else:\n",
        "#             # For larger files, use upload_large with the same binary stream\n",
        "#             csv_buffer.seek(0)  # Ensure pointer is at the beginning\n",
        "#             upload_result = cloudinary.uploader.upload_large(\n",
        "#                 csv_buffer,\n",
        "#                 resource_type=\"raw\",\n",
        "#                 folder=\"processed_csvs\"\n",
        "#             )\n",
        "# ################################################################################################################################################\n",
        "#         # data generation\n",
        "#         # Define color palettes\n",
        "#         DARK_COLORS = [\"#1A0D26\", \"#351B4B\", \"#4F2871\", \"#693696\"]\n",
        "#         LIGHT_COLORS = [\"#9C69C9\", \"#B58ED7\", \"#CEB4E4\", \"#E6D9F2\"]\n",
        "#         images = []\n",
        "#         images.extend(plot_top_categorical_counts_with_dynamic_insights(df_cleaned))\n",
        "#         images.extend(plot_top_numerical_insights(df_cleaned))\n",
        "#         images.extend(generate_summary_report_image(df_cleaned))\n",
        "#         images.extend(plot_kde_with_insights(df_cleaned))\n",
        "#         images.extend(plot_correlation_matrix(df_cleaned))\n",
        "#         forecast_images = forecast_business_metrics(df_cleaned)\n",
        "#         if forecast_images:\n",
        "#             images.extend(forecast_images)\n",
        "#         return {\n",
        "#             \"message\": \"Analysis completed successfully\",\n",
        "#             \"images\": images,\n",
        "#             \"cleaned_csv\": upload_result[\"secure_url\"]\n",
        "#         }\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error processing file: {str(e)}\")\n",
        "#         print(f\"Traceback: {traceback.format_exc()}\")\n",
        "#         raise HTTPException(\n",
        "#             status_code=500,\n",
        "#             detail={\n",
        "#                 \"error\": str(e),\n",
        "#                 \"traceback\": traceback.format_exc()\n",
        "#             }\n",
        "#     )\n",
        "\n",
        "# # Setup ngrok\n",
        "# ngrok.set_auth_token(\"2swgwcEJ5hsXEst7a5WBLtv58s8_5FtZDTirtSBKrSL4e8HUR\")  # Replace with your ngrok auth token\n",
        "\n",
        "# # Run the FastAPI app\n",
        "# if __name__ == \"__main__\":\n",
        "#   public_url = ngrok.connect(8000, bind_tls=True).public_url\n",
        "#   print(f\"FastAPI is publicly accessible at: {public_url}\")\n",
        "#   uvicorn.run(\n",
        "#       \"__main__:app\",\n",
        "#       host=\"0.0.0.0\",\n",
        "#       port=8000,\n",
        "#       log_config=None,  # Disable Uvicorn's default logging\n",
        "#       access_log=False  # Disable access logs\n",
        "#   )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_excel('./Reviews.xlsx')\n",
        "\n",
        "\n",
        "# # Data cleaning\n",
        "# df.dropna(subset=['review', 'sentiment'], inplace=True)\n",
        "# df.drop_duplicates(subset=['review'], inplace=True)\n",
        "\n",
        "#      # Balance dataset\n",
        "# min_count = df['sentiment'].value_counts().min()\n",
        "# df = df.groupby('sentiment').sample(n=min_count, random_state=42)\n",
        "# df = shuffle(df, random_state=42).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# def detect_sarcasm(text):\n",
        "#     sarcasm_keywords = [\n",
        "#         # Common phrases\n",
        "#         \"just what i needed\", \"truly revolutionary\", \"absolutely nothing\",\n",
        "#         \"how wonderful\", \"i love how it\", \"what a surprise\", \"exactly what i hoped for\",\n",
        "#         \"nailed it\", \"couldn't be worse\", \"brilliant work\", \"flawless fail\", \"amazing crash\",\n",
        "#         # Ironic phrases\n",
        "#         \"great job crashing\", \"awesome bug\", \"smooth experience (not)\", \"fantastic... not\",\n",
        "#         \"because why not\", \"works like a charm\", \"perfectly useless\", \"so helpful\", \"thanks a lot\",\n",
        "#         # Contradictions\n",
        "#         \"beautiful design but\", \"sleek and fast but\", \"fast but useless\", \"love it when it fails\",\n",
        "#         \"excellent bug collection\", \"fails beautifully\", \"didn't expect much and still disappointed\",\n",
        "#         # Tone indicators\n",
        "#         \"as if\", \"yeah right\", \"sure thing\", \"whatever\", \"big surprise\"\n",
        "#     ]\n",
        "\n",
        "#     # Check for exaggerated positive words in negative context\n",
        "#     positive_words = [\"amazing\", \"awesome\", \"fantastic\", \"perfect\", \"brilliant\", \"wonderful\"]\n",
        "#     negative_context = [\"crash\", \"fail\", \"bug\", \"slow\", \"broken\", \"useless\", \"terrible\"]\n",
        "\n",
        "#     text_lower = text.lower()\n",
        "\n",
        "#     # Check for sarcasm keywords\n",
        "#     if any(phrase in text_lower for phrase in sarcasm_keywords):\n",
        "#         return True\n",
        "\n",
        "#     # Check for exaggerated positive words in negative context\n",
        "#     if any(word in text_lower for word in positive_words) and any(word in text_lower for word in negative_context):\n",
        "#         return True\n",
        "\n",
        "#     # Check for contrastive conjunctions (but, however, although)\n",
        "#     if re.search(r'\\b(but|however|although|yet)\\b', text_lower, re.IGNORECASE):\n",
        "#         return True\n",
        "\n",
        "#     return False\n",
        "# df['is_sarcastic'] = df['review'].apply(detect_sarcasm)\n",
        "# def preprocess_text(text):\n",
        "#     # Lowercase and remove punctuation (keeping emoticons)\n",
        "#     text = text.lower()\n",
        "#     text = re.sub(r'[^\\w\\s\\'!?]', '', text)\n",
        "\n",
        "#     # Tokenization\n",
        "#     tokens = word_tokenize(text)\n",
        "\n",
        "#     # Custom stopwords removal (preserving negation words)\n",
        "#     stop_words = set(stopwords.words('english'))\n",
        "#     exclusion_words = ['not','no', 'never', 'none', 'nobody', 'nothing', 'neither',\n",
        "#                       'nowhere', 'hardly', 'scarcely', 'barely', 'doesnt', 'dont',\n",
        "#                       'cant', 'couldnt', 'wont', 'wouldnt', 'shouldnt', 'isnt',\n",
        "#                       'wasnt', 'werent', 'hasnt', 'hadnt']\n",
        "\n",
        "#     for word in exclusion_words:\n",
        "#         stop_words.discard(word)\n",
        "\n",
        "#     tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "#     # Lemmatization with POS tagging\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "#     pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "#     lemmatized_tokens = []\n",
        "#     for word, tag in pos_tags:\n",
        "#         pos = get_wordnet_pos(tag)\n",
        "#         lemmatized_tokens.append(lemmatizer.lemmatize(word, pos))\n",
        "\n",
        "#     return ' '.join(lemmatized_tokens)\n",
        "\n",
        "\n",
        "\n",
        "# def get_wordnet_pos(tag):\n",
        "#     if tag.startswith('J'): return wordnet.ADJ\n",
        "#     elif tag.startswith('V'): return wordnet.VERB\n",
        "#     elif tag.startswith('N'): return wordnet.NOUN\n",
        "#     elif tag.startswith('R'): return wordnet.ADV\n",
        "#     else: return wordnet.NOUN\n",
        "# df['processed_review'] = df['review'].apply(preprocess_text)\n",
        "\n",
        "# # Map sentiments to numerical labels\n",
        "# label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
        "# inv_label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "# df['label'] = df['sentiment'].map(label_map)\n",
        "# # Feature extraction with TF-IDF (including bigrams)\n",
        "# vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
        "# X = vectorizer.fit_transform(df['processed_review'])\n",
        "# y = df['label']\n",
        "# X_train, X_test, y_train, y_test, sarcasm_train, sarcasm_test = train_test_split(\n",
        "#     X, y, df['is_sarcastic'], test_size=0.2, random_state=42)\n",
        "# model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "# model.fit(X_train, y_train)\n",
        "# raw_preds = model.predict(X_test)\n",
        "# final_preds = []\n",
        "# for pred, is_sarcastic in zip(raw_preds, sarcasm_test.tolist()):\n",
        "#     if is_sarcastic and pred != 0:  # If sarcastic and not already negative\n",
        "#         final_preds.append(0)  # Force to negative\n",
        "#     else:\n",
        "#         final_preds.append(pred)\n",
        "# def predict_sentiment():\n",
        "#     print(\"\\nEnter your review (type 'exit' to stop):\")\n",
        "#     while True:\n",
        "#         user_review = input(\"Your Review: \")\n",
        "#         if user_review.lower() == 'exit':\n",
        "#             break\n",
        "\n",
        "#         # Detect sarcasm\n",
        "#         is_sarcastic = detect_sarcasm(user_review)\n",
        "\n",
        "#         # Preprocess text\n",
        "#         processed_text = preprocess_text(user_review)\n",
        "#         features = vectorizer.transform([processed_text])\n",
        "\n",
        "#         # Predict\n",
        "#         pred = model.predict(features)[0]\n",
        "\n",
        "#         # Adjust for sarcasm\n",
        "#         if is_sarcastic and pred != 0:\n",
        "#             pred = 0\n",
        "\n",
        "#         # Get confidence scores\n",
        "#         confidence_scores = model.predict_proba(features)[0]\n",
        "#         confidence = round(confidence_scores[pred] * 100, 2)\n",
        "\n",
        "#         # Get sentiment label\n",
        "#         label = inv_label_map[pred]\n",
        "\n",
        "#         # Print results\n",
        "#         print(f\"→ Prediction: {label} (confidence: {confidence}%)\")\n",
        "#         print(f\"→ Sarcasm detected: {is_sarcastic}\")\n",
        "\n",
        "#         # If sarcastic, show original prediction\n",
        "#         if is_sarcastic:\n",
        "#             original_pred = model.predict(features)[0]\n",
        "#             print(f\"→ Original prediction (before sarcasm adjustment): {inv_label_map[original_pred]}\")\n",
        "\n",
        "#         print()\n",
        "\n",
        "\n",
        "# predict_sentiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Nvdyz2oSGSOD",
        "outputId": "f154b2ee-723b-46b0-a065-09844c47d587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter your review (type 'exit' to stop):\n",
            "Your Review: excellent\n",
            "→ Prediction: positive (confidence: 59.0%)\n",
            "→ Sarcasm detected: False\n",
            "\n",
            "Your Review: ok\n",
            "→ Prediction: negative (confidence: 37.63%)\n",
            "→ Sarcasm detected: False\n",
            "\n",
            "Your Review: AWESOME APP\n",
            "→ Prediction: positive (confidence: 55.44%)\n",
            "→ Sarcasm detected: False\n",
            "\n",
            "Your Review: BAD APP\n",
            "→ Prediction: positive (confidence: 37.98%)\n",
            "→ Sarcasm detected: False\n",
            "\n",
            "Your Review: nice\n",
            "→ Prediction: neutral (confidence: 35.38%)\n",
            "→ Sarcasm detected: False\n",
            "\n",
            "Your Review: NICEEE\n",
            "→ Prediction: negative (confidence: 37.63%)\n",
            "→ Sarcasm detected: False\n",
            "\n",
            "Your Review: AWESOMEEE\n",
            "→ Prediction: negative (confidence: 37.63%)\n",
            "→ Sarcasm detected: False\n",
            "\n",
            "Your Review: awesome app\n",
            "→ Prediction: positive (confidence: 55.44%)\n",
            "→ Sarcasm detected: False\n",
            "\n",
            "Your Review: awesome application\n",
            "→ Prediction: positive (confidence: 46.06%)\n",
            "→ Sarcasm detected: False\n",
            "\n",
            "Your Review: NICE\n",
            "→ Prediction: neutral (confidence: 35.38%)\n",
            "→ Sarcasm detected: False\n",
            "\n",
            "Your Review: NICEEE\n",
            "→ Prediction: negative (confidence: 37.63%)\n",
            "→ Sarcasm detected: False\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1e8d727ef464>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m \u001b[0mpredict_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-1e8d727ef464>\u001b[0m in \u001b[0;36mpredict_sentiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEnter your review (type 'exit' to stop):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0muser_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your Review: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_review\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}